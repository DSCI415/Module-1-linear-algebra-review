{"title":"1.3 - covariance and eigendecomposition","markdown":{"yaml":{"title":"1.3 - covariance and eigendecomposition","format":{"revealjs":{"slide-number":true,"smaller":true}},"editor_options":{"chunk_output_type":"console"}},"headingText":"Case study","containsRefs":false,"markdown":"\n\n\nAs our case study, we're going to use some simulated data.\n\n```{r}\n#| echo: true\n#| warning: false\n#| message: false\n\nlibrary(MASS)\n\n# parameters\nmu <- c(4, 3)\nsig1 <- 1\nsig2 <- 3\nrho <- 0.8\nn <- 50\n\n# Covariance matrix\n\nSigma <- matrix(c(sig1^2, rho*sig1*sig2,\n                  rho*sig1*sig2, sig2^2), 2, 2)\n\n# Simulate n observations\nset.seed(555)  # for reproducibility\ndata <- mvrnorm(n=n, mu=mu, Sigma=Sigma)\n\n# Convert to data frame for plotting\ndf <- data.frame(data)\nnames(df) <- c('x','y')\nhead(df)\n```\n\n## Goal\n\nConsider plot of the data below:\n\n```{r}\n#| fig-align: center\n#| fig-width: 5\n#| fig-asp: 1\n#| warning: false\n#| message: false\n\nlibrary(patchwork)\nlibrary(tidyverse)\n\nlim <- c(-10,10)\n\nbase <- ggplot() +\n  scale_x_continuous(breaks=seq(min(lim), max(lim), by=1),limits=lim) +\n    scale_y_continuous(breaks=seq(min(lim), max(lim), by=1),limits=lim) + \n  geom_vline(aes(xintercept = 0)) +  geom_hline(aes(yintercept = 0)) +  \n  theme_minimal(base_size = 14) + \n  theme(panel.grid.minor = element_blank()) + \n  labs(x='', y='')\n\ndf_centered <- scale(df, scale = FALSE)\n\nevecs <- -eigen(cov(df))$vectors\nevals <- eigen(cov(df))$values\ndf_rotated <- data.frame(df_centered%*%evecs)\nnames(df_rotated) <- c('x','y')\n\n\np1 <- base+ geom_point(aes(x = x, y = y), data = df, size = 3, alpha = 0.8) + ggtitle('Data')\np2 <- base+ geom_point(aes(x = x, y = y), data = df_centered, size = 3, alpha = 0.8) + ggtitle('Center')\np3 <- base+ geom_point(aes(x = x, y = y), data = df_rotated, size = 3, alpha = 0.8) + ggtitle('Rotate')\n\np1\n```\n\nSuppose we want to:\n\n1.  Center the data on the origin;\n2.  Rotate it so that:\n\n-   it is no longer correlated;\n-   maximal variability is along the first (horizontal) dimension\n\n## Visual goal\n\n```{r}\n#| fig-width: 12\n#| fig-height: 4\np1+p2 +p3\n```\n\nThis is one of the fundamental ideas behind *principal component analysis*, which we'll cover in much more detail later on.\n\n## Defining new axes\n\nConceptually, the way we accomplish this is not by transforming the *points*, but by *defining new axes*:\n\n```{r}\n#| fig-width: 6\n#| fig-height: 6\n#| fig-align: center\nmeanvec <- apply(df, 2, mean)\n\nmult <- 5\nnewaxis <- p1 + \n  geom_segment(aes(x = meanvec[1], xend = meanvec[1]+mult*evecs[1,1],\n                   y = meanvec[2], yend = meanvec[2]+mult*evecs[2,1]),col='red') + \n    geom_segment(aes(x = meanvec[1], xend = meanvec[1]-mult*evecs[1,1],\n                   y = meanvec[2], yend = meanvec[2]-mult*evecs[2,1]),col='red') + \n    geom_segment(aes(x = meanvec[1], xend = meanvec[1]+mult*evecs[1,2],\n                   y = meanvec[2], yend = meanvec[2]+mult*evecs[2,2]),col='red') + \n    geom_segment(aes(x = meanvec[1], xend = meanvec[1]-mult*evecs[1,2],\n                   y = meanvec[2], yend = meanvec[2]-mult*evecs[2,2]),col='red') + \n  ggtitle('')\nnewaxis\n```\n\n... so how do we do this?\n\n## The mean vector\n\nThe first part is easy.\n\nThe coordinates of the new axis origin are simply the mean vector:\n\n```{r}\n#| echo: true\nmeanvec <- apply(df, 2, mean)\nmeanvec\n```\n\n```{r}\n#| fig-width: 6\n#| fig-height: 6\n#| fig-align: center\n\nnewaxis + \n  geom_point(aes(x =  meanvec[1], y =  meanvec[2]), size = 5, pch = 19, col='red') \n```\n\n## The covariance matrix\n\n-   The next ingredient is the *covariance matrix.*\\\n-   For $p=2$ dimensions, this measures:\n    -   Horizontal variability;\n    -   Vertical variability;\n    -   Covariance\n\n## Horizontal and vertical variance\n\nHorizontal and vertical variances are also known as ***marginal variances***:\n\n```{r}\n#| fig-width: 15\n#| fig-height: 5\nh <- base + geom_point(aes(x = x, y = rep(0,nrow(df))), size=3, data = df, alpha = 0.8) + ggtitle('Horizontal variability') + geom_point(aes(x = meanvec[1],y=0), size = 5, pch=19,col='red')\nv <- base + geom_point(aes(y = y, x = rep(0,nrow(df))), size =3,data = df, alpha = 0.8) + ggtitle('Vertical variability') + geom_point(aes(y = meanvec[2],x=0), size = 5, pch=19,col='red')\np1withmean <- p1 +   geom_point(aes(x =  meanvec[1], y =  meanvec[2]), size = 5, pch = 19, col='red') \n\np1withmean + h + v\n```\n\n. . .\n\nGiven mean-centered $n\\times 1$ column vectors $x_c$ and $y_c$:\n\n-   Horizontal marginal variability: $\\frac{1}{n-1}x_c^T x_c$\n-   Vertical marginal variability: $\\frac{1}{n-1}y_c^T y_c$\n\n## Computing horizontal and vertical variability {auto-animate=\"true\"}\n\n```{r}\n#| echo: true\n# Mean-center the columns:\ndf_centered <- scale(df, center = TRUE, scale = FALSE)\n```\n\n## Computing horizontal and vertical variability {auto-animate=\"true\"}\n\n```{r}\n#| echo: true\n# Mean-center the columns:\ndf_centered <- scale(df, center = TRUE, scale = FALSE)\n\n#Horizontal variability:\nx_c <- df_centered[,1]\nt(x_c) %*% x_c / (n-1)\n```\n\n## Computing horizontal and vertical variability {auto-animate=\"true\"}\n\n```{r}\n#| echo: true\n# Mean-center the columns:\ndf_centered <- scale(df, center = TRUE, scale = FALSE)\n\n#Horizontal variability:\nx_c <- df_centered[,1]\nt(x_c) %*% x_c / (n-1)\n\n#Vertical variability:\ny_c <- df_centered[,2]\nt(y_c) %*% y_c/ (n-1)\n```\n\nDoes this match our visual intuition?\n\n## Covariance\n\n::::: columns\n::: {.column width=\"50%\"}\n-   *Covariance* looks at the mean-centered data:\n\n```{r}\n#| fig-width: 6\n#| fig-height: 6\n#| fig-align: center\np2 + ggtitle('') \n```\n:::\n\n::: {.column .fragment width=\"50%\"}\n-   Then considers whether the $(x,y)$ pairs are above or below their means:\n\n```{r}\n#| fig-width: 6\n#| fig-height: 6\n#| fig-align: center\npos <- p2 + ggtitle('') + \n  geom_text(aes(x = 8, y=8, label = '(+, +)'), size =10)+\n  geom_text(aes(x = -8, y=8, label = '(-, +)'), size =10)+\n  geom_text(aes(x = -8, y=-8, label = '(-, -)'), size =10)+\n  geom_text(aes(x = 8, y=-8, label = '(+, -)'), size =10)\npos\n```\n:::\n:::::\n\n## Covariance\n\n-   Large $(+,+)$ and $(-,-)$ pairs contribute to large *positive* covariance\n-   Large $(+,-)$ and $(-,+)$ pairs contribute to large *negative* covariance\n\n```{r}\n#| fig-width: 15\n#| fig-height: 5\n#| fig-align: center\n\nrho2 <- -0.8\nrho3 <- 0\nSigma2 <- matrix(c(sig1^2, rho2*sig1*sig2,\n                  rho2*sig1*sig2, sig2^2), 2, 2)\nSigma3 <- matrix(c(sig1^2, rho3*sig1*sig2,\n                  rho3*sig1*sig2, sig2^2), 2, 2)\n\n# Simulate n observations\nset.seed(523)  \ndf2 <- data.frame(scale(mvrnorm(n=n, mu=mu, Sigma=Sigma2),scale=FALSE))\ndf3 <- data.frame(scale(mvrnorm(n=n, mu=mu, Sigma=Sigma3),scale=FALSE))\nnames(df2) <- names(df3) <- c('x','y')\n\nneg <- base+ geom_point(aes(x = x, y =y), data = df2, size = 3, alpha = 0.8)+\n  geom_text(aes(x = 8, y=8, label = '(+, +)'), size =10)+\n  geom_text(aes(x = -8, y=8, label = '(-, +)'), size =10)+\n  geom_text(aes(x = -8, y=-8, label = '(-, -)'), size =10)+\n  geom_text(aes(x = 8, y=-8, label = '(+, -)'), size =10) + \n  ggtitle(\"Lots of large (-,+) and (+,-) pairs\")\nzero <- base+ geom_point(aes(x = x, y =y), data = df3, size = 3, alpha = 0.8)+\n  geom_text(aes(x = 8, y=8, label = '(+, +)'), size =10)+\n  geom_text(aes(x = -8, y=8, label = '(-, +)'), size =10)+\n  geom_text(aes(x = -8, y=-8, label = '(-, -)'), size =10)+\n  geom_text(aes(x = 8, y=-8, label = '(+, -)'), size =10) + \n  ggtitle(\"(-,+) and (+,-) pairs balance the \n          (+,+) and (-,-) pairs\")\n\npostitle <- pos + \n  ggtitle(\"Lots of large (-,-) and (+,+) pairs\")\npostitle+neg+zero\n```\n\n## Computing covariance\n\nTo compute covariance: $\\frac{1}{n-1}x_c^T y_c$\n\n```{r}\n#| echo: true\nt(x_c)%*% y_c/ (n-1)\n```\n\n. . .\n\nAll ingredients (horizontal variance, vertical variance, and covariance) are often included in the *covariance matrix*, frequently notated by $\\Sigma$:\n\n$$ \\Sigma = \\begin{pmatrix} \\mbox{Horizontal variance} & \\mbox{Covariance} \\\\ \\mbox{Covariance} & \\mbox{Vertical variance} \\end{pmatrix}$$\n\n. . .\n\n```{r}\n#| echo: true\n#| attr-source: \"style='font-size: 2em'\"\n#| attr-output: \"style='font-size: 1em'\"\n\nSigma <- cov(df)\nSigma\n```\n\n## Covariance interpretation \n\nLet's take another look at this matrix:\n\n```{r}\n#| echo: true\n#| attr-source: \"style='font-size: 2em'\"\n#| attr-output: \"style='font-size: 1em'\"\nSigma\n```\n\n. . .\n\nSum of diagonal: ***total information in data*** (horizontal + vertical variance)\n\n```{r}\n#| echo: true\n#| attr-source: \"style='font-size: 2em'\"\n#| attr-output: \"style='font-size: 1.5em'\"\nsum(diag(Sigma))\n```\n\n. . .\n\nOff-diagonal: ***information in common between columns***\n```{r}\n#| attr-output: \"style='font-size: 1.5em'\"\nSigma[1,2]\n```\n\n## Quiz question\n\nMatch the covariance matrices to the scatterplot.\n\n![](images/clipboard-1080268104.png){fig-align=\"center\" width=\"400\"}\n\n![](images/clipboard-3029979869.png){width=\"500\" fig-align=\"center\"}\n\n\n\n## Eigen decomposition\n\n::: {.column}\n\n- We know where the new axis origin is:\n\n```{r .fragment}\n#| fig-width: 3\n#| fig-asp: 1\n#| fig-align: center\n\np1withmean + ggtitle('')\n```\n\n::: {.fragment}\n- We have the $2 \\times 2$ covariance matrix $\\Sigma$ that tells us about vertical variance, horizontal variance, and covariance:\n\n```{r}\nSigma\n```\n:::\n\n::: \n\n::: {.column .fragment}\n- What we need are the *vectors* that start from the new origin to form our new axis!\n\n\n```{r}\n#| fig-width: 4\n#| fig-asp: 1\n#| fig-align: center\n\nmult <- 2\neigenplot <- p1withmean + ggtitle('') + \n  geom_segment(aes(x = meanvec[1], \n                   xend = meanvec[1]+mult*evecs[1,1],\n                   y = meanvec[2], \n                   yend = meanvec[2] + mult*evecs[2,1]),col='red',\n               arrow = arrow(length = unit(0.1, \"cm\")), linewidth = 1) + \n    geom_segment(aes(x = meanvec[1], \n                     xend = meanvec[1]+mult*evecs[1,2],\n                   y = meanvec[2], \n                   yend = meanvec[2]+mult*evecs[2,2]),col='red',\n                 arrow = arrow(length = unit(0.1, \"cm\")), linewidth = 1)\n\neigenplot\n```\n:::\n\n## Eigenvector\n\n- Consider a $p\\times p$  matrix $A$.\n- A $p$-vector $v$ is an *eigenvector* of $A$ if, for a scalar $\\lambda$:\n\n$$A v = \\lambda v$$\n\n- In other words, eigenvectors are vectors that $A$ merely *shrinks* or *elongates*.  \n\n## Is it an eigenvector?\n\n- Consider the matrix $A = \\begin{pmatrix} 2 & 1 \\\\ 0 & 3 \\end{pmatrix}$.\n\n- Which of these are eigenvectors of $A$? \n\n$$ v_1 = \\begin{pmatrix} 1 \\\\0 \\end{pmatrix}; \\ \\ \\ \\ \nv_2 = \\begin{pmatrix} 0 \\\\1 \\end{pmatrix}; \\ \\ \\ \\ \nv_3 = \\begin{pmatrix} 2 \\\\2 \\end{pmatrix}$$\n$$v_4 = \\begin{pmatrix} 0.5 \\\\ 0.5 \\end{pmatrix};\\ \\ \\ \nv_5 = \\begin{pmatrix} 2 \\\\ 4 \\end{pmatrix};\\ \\ \\ \nv_6 = \\begin{pmatrix} 1/3 \\\\ 0 \\end{pmatrix}$$\n\n\n\n## Finding eigenvalues\n\n- There are infinitely many eigenvectors for a matrix $A$, but at most $p$ eigenvalues.\n- Thus, eigendecomposition starts with finding the candidate *eigenvalues*.  \n- An *eigenvalue* of $A$, a scalar notated $\\lambda$, satisfies the equation: \n\n\n$$det(A - \\lambda I) = 0$$ \n\nthis is sometimes referred to as the *characteristic polynomial*.  \n\n## Finding eigenvalues\n\n- Consider matrix $A = \\begin{pmatrix} 2 & 1 \\\\ 0 & 3 \\end{pmatrix}$.\n\n- Then:\n\n$$A  - \\lambda I = \\begin{pmatrix} 2 & 1 \\\\ 0 & 3 \\end{pmatrix}  - \\begin{pmatrix} \\lambda & 0 \\\\ 0& \\lambda \\end{pmatrix} $$\n\n\n. . .\n\n$$ =  \\begin{pmatrix} 2-\\lambda & 1 \\\\ 0 & 3-\\lambda \\end{pmatrix} $$\n\n. . .\n\n$$det(A-\\lambda I) = (2-\\lambda)(3-\\lambda) = 0$$\n\n. . . \n\n- Solutions: $\\lambda = 3$ or $\\lambda = 2$\n\n## Finding eigenvectors\n\n- Eigenvalues of $A = \\begin{pmatrix} 2 & 1 \\\\ 0 & 3 \\end{pmatrix}$ are $\\lambda = 2$ or $\\lambda = 3$.\n\n- Thus the *eigenvectors* of $A$ are vectors that $A$ either *doubles* or *triples* in magnitude!\n\n. . .\n\n- Reconsider the eigenvectors we identified earlier:\n\n\n$$ v_1 = \\begin{pmatrix} 1 \\\\0 \\end{pmatrix}; \\ \\ \\ \\ \nv_3 = \\begin{pmatrix} 2 \\\\2 \\end{pmatrix}; \\ \\ \\ \\ \nv_4 = \\begin{pmatrix} 0.5 \\\\ 0.5 \\end{pmatrix};\\ \\ \\ \nv_6 = \\begin{pmatrix} 1/3 \\\\ 0 \\end{pmatrix}$$\n\nWhich ones does $A$ double?  Which ones does $A$ triple? \n\n\n## Finding eigenvectors for $\\lambda = 3$ \n\nTo find eigenvectors for $\\lambda = 3$ (ones that $A$ triples), recall eigenvectors satisfy:\n\n$$Av = \\lambda v = 3v$$\n\n. . .\n\nIn matrix form: \n\n$$\\begin{pmatrix} 2 & 1 \\\\ 0 & 3 \\end{pmatrix}\\begin{pmatrix} x \\\\ y\\end{pmatrix} = 3 \\begin{pmatrix} x\\\\ y \\end{pmatrix}$$\n\n. . .\n\n$$ \\begin{pmatrix} 2x+y  \\\\ 0x + 3y \\end{pmatrix} = \\begin{pmatrix}3x \\\\ 3y \\end{pmatrix}$$\n\n. . . \n\n$$x = 1; y = 1$$\n\n. . .\n\nThus $v = \\begin{pmatrix} 1\\\\ 1 \\end{pmatrix},$ or any multiple of $v$, is an eigenvector that $A$ triples!\n\n\n\n## Finding eigenvectors for $\\lambda = 2$ \n\nTo find eigenvectors for $\\lambda = 2$ (ones that $A$ doubles), recall eigenvectors satisfy:\n\n$$Av = \\lambda v = 2v$$\n\n. . .\n\nIn matrix form: \n\n$$\\begin{pmatrix} 2 & 1 \\\\ 0 & 3 \\end{pmatrix}\\begin{pmatrix} x \\\\ y\\end{pmatrix} = 2 \\begin{pmatrix} x\\\\ y \\end{pmatrix}$$\n\n. . .\n\n$$ \\begin{pmatrix} 2x+y  \\\\ 0x + 3y \\end{pmatrix} = \\begin{pmatrix}2x \\\\ 2y \\end{pmatrix}$$\n\n. . . \n\n$$x = 1; y = 0$$\n\n. . .\n\nThus $v = \\begin{pmatrix} 1\\\\ 0 \\end{pmatrix},$ or any multiple of $v$, is an eigenvector that $A$ doubles!\n\n\n## Eigenvectors of symmetric matrices are orthogonal !\n\n- If we are decomposing a *symmetric* $p\\times p$ matrix $B$, then eigenvectors for different eigenvalues are orthogonal!\n- Recall that orthogonal (i.e., perpendicular) vectors have dot product = 0\n\n. . .\n\n- Recall eigenvectors for $A =\\begin{pmatrix} 2 & 1 \\\\ 0 & 3 \\end{pmatrix}$:  $v_1=\\begin{pmatrix} 1\\\\ 0 \\end{pmatrix}$ and $v_2 =  \\begin{pmatrix} 1\\\\ 1 \\end{pmatrix}$.  Are they orthogonal?  Why/why not?\n\n. . .\n\n- The orthogonality of eigenvectors of symmetric matrices is important for decomposing covariance matrices! (coming up)\n\n## Eigenbases\n\n- For $\\lambda = 3$, $\\begin{pmatrix} 1  \\\\ 1 \\end{pmatrix}$ *or any multiple thereof* is an eigenvector.\n\n- For $\\lambda = 2$, $\\begin{pmatrix} 1  \\\\ 0 \\end{pmatrix}$ *or any multiple thereof* is an eigenvector.\n\n- Thus for any eigenvalue there are infinitely many eigenvectors.\n\n- *Eigenbases* are unique, normed (length-1) vectors for each eigenvalue\n\n$$v_{norm} = \\frac{v}{||v||_2}$$\n\n\n## Eigenbase for $\\lambda = 3$\n\nL2-normed  $\\lambda = 3$ eigenvector $\\begin{pmatrix} 1  \\\\ 1 \\end{pmatrix}$:\n\n$$ \\sqrt{1^2 + 1^2} = \\sqrt{2}$$\n\nNormed $\\lambda = 3$ eigenvector: \n\n$$\\begin{pmatrix} 1/\\sqrt{2}  \\\\ 1/\\sqrt{2} \\end{pmatrix} =\\begin{pmatrix} 0.7071068  \\\\ 0.7071068 \\end{pmatrix} $$\n\n\n## Eigenbase for $\\lambda = 2$\n\nL2-normed  $\\lambda = 2$ eigenvector $\\begin{pmatrix} 1  \\\\ 0 \\end{pmatrix}$:\n\n$$ \\sqrt{1^2 + 0^2} = \\sqrt{1}$$\n\nSo $\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ already has L2 norm = 1! \n\n## `eigen` in `R`\n\nTo find eigenvalues and eigenbases in `R`:\n\n```{r}\n#| echo: true\n#| attr-source: \"style='font-size: 2em'\"\n#| attr-output: \"style='font-size: 1em'\"\n\nA <- matrix(c(2,1,\n              0,3),\n            2,2, byrow=TRUE)\neigen(A)\n```\n\n## Tying it all together\n\n- So we have this covariance matrix (which, by the way, is symmetric, as all covariance matrices will be):\n\n\n```{r}\n#| echo: true\nSigma\n```\n\n. . .\n\n-  The *eigenbases* of $\\Sigma$ are the new axes we want!!\n- And, since $\\Sigma$ is symmetric, they are guaranteed orthogonal (so will work as new axes).\n\n```{r}\n#| echo: true\neig_decomposition <- eigen(Sigma)\neig_decomposition\n```\n\n## Plotting the bases\n\n```{r}\n#| echo: true\n#| fig-width: 6\n#| fig-asp: 1\n#| fig-align: center\neigenvectors <- eig_decomposition$vectors\n\nvectordf <- data.frame(t(eigenvectors)) %>% \n  mutate(mean_x = meanvec[1], mean_y = meanvec[2]) %>% \n  rename(evector_xdirection = X1, evector_ydirection = X2)\n\nvectordf\neigenbaseplot <- base + \n  geom_point(aes(x = x, y = y), data = df, alpha = 0.8, size = 3) + \n  geom_point(aes(x = mean_x,  y = mean_y), data = vectordf, color='red',size = 4) + \n  geom_segment(aes(x = mean_x, y = mean_y, \n                   xend = mean_x +evector_xdirection, \n                   yend = mean_y + evector_ydirection), \n               color='red', data = vectordf,  \n               arrow = arrow(length = unit(0.2, \"cm\")), linewidth = 1) \neigenbaseplot\n```\n\n## Scaling the bases\n\nIn this plot we see the *directions* of the axes:\n\n```{r}\n#| fig-width: 6\n#| fig-asp: 1\n#| fig-align: center\n#| \neigenbaseplot\n```\n\n. . .\n\nThe eigenvalues give us a sense as to how much variability is in the *direction* of each axis:\n\n```{r}\neigenvalues <- eig_decomposition$values\neigenvalues\n```\n\n. . .\n\nThese values indicate there is a lot more variability in the $e_1$ (first eigenvector) direction than in the $e_2$ (second eigenvector) direction.\n\n## Scaling the bases\n\nWe can scale the axes accordingly by the square root of the eigenvalues:\n\n::: column\n```{r}\n#| echo: true\nvectordf <- data.frame(t(eigenvectors)) %>% \n  mutate(mean_x = meanvec[1], mean_y = meanvec[2]) %>% \n  rename(evector_xdirection = X1, evector_ydirection = X2) %>% \n  mutate(evector_xdirection_scaled = sqrt(eigenvalues)*evector_xdirection,\n         evector_ydirection_scaled = sqrt(eigenvalues)*evector_ydirection)\n\nvectordf\n```\n:::\n\n::: column\n```{r}\n#| fig-width: 6\n#| fig-asp: 1\n#| fig-align: center\nbase + \n  geom_point(aes(x = x, y = y), data = df, alpha = 0.8, size = 2) + \n  geom_point(aes(x = mean_x,  y = mean_y), data = vectordf, color='red',size = 2) + \n  geom_segment(aes(x = mean_x, y = mean_y, \n                   xend = mean_x +evector_xdirection_scaled, \n                   yend = mean_y + evector_ydirection_scaled), \n               color='red', data = vectordf,  \n               arrow = arrow(length = unit(0.2, \"cm\")), linewidth = 1) \n```\n:::\n\n## Interpreting the eigenvalues\n\n- The eigenvalues also have meaningful interpretations in terms of variability. \n\n- Recall:\n\n```{r}\n#| echo: true\nSigma\nsum(diag(Sigma))\n```\n\n... so the total horizontal and vertical variance is `r sum(diag(Sigma))`.\n\n. . .\n\nNow consider the eigenvalues:\n\n```{r}\n#| echo: true\neigenvalues\nsum(eigenvalues)\n```\n\n... so the summed eigenvalues *also* equal the total horizontal and vertical variance!!\n\n## Eigenvectors as rotation tools\n\n- Reconsider our mean-centered data:\n\n```{r}\n#| echo: true\ndf_centered <- scale(df, center = TRUE, scale = FALSE)\n```\n\n```{r}\n#| fig-width: 6\n#| fig-asp: 1\n#| fig-align: center\np2 + ggtitle('')\n```\n\n\n## Eigenvectors as rotation tools\n\n- Multiplying the centered data by the eigenvectors yields the *rotation* that treats the eigenvectors as the new horizontal and vertical axes:\n\n```{r}\n#| echo: true\ndf_rotated <- data.frame(df_centered %*% eigenvectors)\nnames(df_rotated) <- c('x','y')\n```\n\n```{r}\n#| fig-width: 6\n#| fig-asp: 1\n#| fig-align: center\np3 + ggtitle('')\n```\n\n## Eigenvalues as new covariance diagonals\n\n- Let's look at the covariance matrix of the centered, rotated data:\n\n```{r}\n#| echo: true\ncov(df_rotated) %>% round(4)\n```\n\n. . .\n\n- Hopefully these look familiar:\n\n```{r}\n#| echo: true\neigenvalues\n```\n\n- Thus the eigenvalues tell us the total horizontal and vertical variability of the mean centered data rotated to orient along the eigenvectors!\n\n## Summary\n\n- Covariance matrix $\\Sigma$: symmetric $p\\times p$ matrix\n  - Marginal variance of each dimension on the diagonal\n  - Covariance on the off-diagonals\n\n. . .\n\n- Eigenvectors of a square $p\\times p$ matrix $A$\n  - Vectors that $A$ scales by a constant\n  - Can be normalized to determine *eigenbases*\n  - If $A$ is symmetric, eigenvectors of different eigenvalues are orthogonal\n\n. . .\n\n- Eigenvalues\n  - Scalars that tell us how much eigenvectors are scaled by $A$ \n\n## Summary\n\nEigendecomposition of $\\Sigma$: \n\n- Eigenvectors\n  - 1) tell us which directions the reoriented axes go;\n  - 2) give us rules for rotating the data to be uncorrelated and reoriented;\n  - 3) are orthogonal (since $\\Sigma$ is symmetric)\n- Eigenvalues tell us the values of marginal horizontal and vertical variance after rotation\n  \n## Correlation vs covariance\n\n- Consider the following two covariance matrices from two data sets, `df1` and `df2`.\n- In both, we know the covariance but not the marginal (horizontal/vertical) variances:\n\n$$ \\Sigma_1 = \\begin{pmatrix}{? & 100 \\\\ 100 & ?}\\end{pmatrix}\\ \\ \\  \\Sigma_2 = \\begin{pmatrix}{? & 0.1 \\\\ 0.1 & ?}\\end{pmatrix}$$\n\n- Which matrix represents a data with more relationship between the two columns?\n\n\n## Correlation vs covariance\n\n- Consider the following two covariance matrices from two data sets, `df1` and `df2`.\n- In both, we know the covariance but not the marginal (horizontal/vertical) variances:\n\n$$ \\Sigma_1 = \\begin{pmatrix}? & 50 \\\\ 50 & ?\\end{pmatrix};\\ \\ \\  \\Sigma_2 = \\begin{pmatrix}? & 0.1 \\\\ 0.1 & ?\\end{pmatrix}$$\n\n- Which matrix represents a dataset with more relationship between the two columns?\n\n## Plotting the two data sets\n\n```{r}\n#| fig-width: 10\n#| fig-height: 5\n#| fig-align: center\n\n\nmu <- c(0,0)\nsigx <- 10; sigy <- 50; rho <- 0.1\nSigma1 <- matrix(c(sigx^2, sigx*sigy*rho,sigx*sigy*rho,sigy^2), nrow=2)\nsigx <- 1/3; sigy <- 1/3; rho <- .9\nSigma2 <- matrix(c(sigx^2, sigx*sigy*rho,sigx*sigy*rho,sigy^2), nrow=2)\n\n# Simulate data\nset.seed(944)\ndf1 <- as.data.frame(mvrnorm(n = 50, mu = c(0,0), Sigma1)); names(df1)=c(\"X\",\"Y\")\ndf2 <- as.data.frame(mvrnorm(n = 50, mu = c(0,0), Sigma2)); names(df2)=c(\"X\",\"Y\")\n\n# Make plots\np1 <- ggplot(df1, aes(X,Y)) + geom_point(alpha=0.6) + ggtitle(\"Plot of df1\") + theme_minimal(base_size = 14) + geom_hline(aes(yintercept = 0)) + geom_vline(aes(xintercept = 0))\np2 <- ggplot(df2, aes(X,Y)) + geom_point(alpha=0.6) + ggtitle(\"Plot of df2\") + theme_minimal(base_size = 14)+  geom_hline(aes(yintercept = 0)) + geom_vline(aes(xintercept = 0))\n\n\n# Arrange side by side\np1+p2\n```\n- Which data set has more relationship?  \n- What differences do you notice about between these two data sets? \n\n## Covariance vs correlation\n\n- Let $\\sigma_1^2$ represent the marginal variance in the first dimension (e.g., horizontal)\n- Let $\\sigma_2^2$ represent the marginal variance in the second dimension (e.g., vertical)\n- The *correlation*, $\\rho$, between two variables $x$ and $y$ is defined as:\n\n$$\\rho = \\frac{Cov(x,y)}{\\sigma_1 \\sigma_2}$$\n\n. . .\n\n- Note that this also implies that $Cov(x,y) = \\rho \\sigma_1 \\sigma_2$; in other words, that the covariance is a function both of the *relationship* between variables and the *marginal* variances!\n\n## Facts about $\\rho$\n\n- Unitless\n- $-1 \\leq \\rho \\leq 1$\n- $|\\rho|$ close to 1 $\\rightarrow$ strong relationship\n- $|\\rho|$ close to 0 $\\rightarrow$ weak relationship\n\n## Covariance - correlation relationship\n\n::: {.column width=\"40%\"}\nFor 2D data:\n\n$$\\Sigma = \\begin{pmatrix}\\sigma_1^2 & \\sigma_1 \\sigma_2 \\rho\\\\\n\\sigma_1 \\sigma_2 \\rho & \\sigma_2^2 \\end{pmatrix}$$\n\n$$R = \\begin{pmatrix}1 &  \\rho\\\\\n\\rho & 1 \\end{pmatrix}$$\n:::\n\n::: {.column .fragment width=\"60%\"}\nFor $p$-dimensional data:\n\n$$\\Sigma = \\begin{pmatrix}\\sigma_1^2 & \\sigma_1 \\sigma_2 \\rho_{12} & \\sigma_1 \\sigma_3 \\rho_{13} & ... & \\sigma_1 \\sigma_p \\rho_{1p}\\\\\n\\sigma_1 \\sigma_2 \\rho_{12} & \\sigma_2^2 & \\sigma_2 \\sigma_3 \\rho_{23} & ... & \\sigma_2 \\sigma_p \\rho_{2p}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\ \n\\sigma_1 \\sigma_p \\rho_{1p} & \\sigma_2\\sigma_p \\rho_{2p} & \\sigma_3 \\sigma_p \\rho_{3p} & ... & \\sigma_p^2\\end{pmatrix}$$\n\n\n$$R = \\begin{pmatrix}1 &  \\rho_{12} & \\rho_{13} & ... & \\rho_{1p}\\\\\n\\rho_{12} & 1 & \\rho_{23} & ... & \\rho_{2p}\\\\ \n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n\\rho_{1p} & \\rho_{2p} & \\rho_{3p} & ... & 1\\end{pmatrix}$$\n:::\n\n## Find the correlation\n\nThe two covariance matrices I used to simulate `df1` and `df2` were:\n\n\n$$ \\Sigma_1 = \\begin{pmatrix}100 & 50 \\\\ 50 & 2500\\end{pmatrix};\\ \\ \\  \\Sigma_2 = \\begin{pmatrix}1/9 & 0.1 \\\\ 0.1 & 1/9\\end{pmatrix}$$\n\n- What is $\\rho_1$, the correlation between the two variables for `df1`?\n- What is $\\rho_2$, the correlation between the two variables for `df2`?\n\n## Simulated data sets\n\n\n::: {.column width=\"60%\"}\nCode I used for simulating the two data sets:\n```{r}\n#| echo: true\n\n#Parameters:\nsigx <- 10; sigy <- 50; rho <- 0.1\nSigma1 <- matrix(c(sigx^2, sigx*sigy*rho,\n                   sigx*sigy*rho,sigy^2), nrow=2)\n\nsigx <- 1/3; sigy <- 1/3; rho <- .9\nSigma2 <- matrix(c(sigx^2, sigx*sigy*rho,\n                   sigx*sigy*rho,sigy^2), nrow=2)\n\n# Simulate data\nlibrary(MASS)\nset.seed(944)\ndf1 <- data.frame(mvrnorm(n = 50, mu = c(0,0), Sigma1))\nnames(df1)=c(\"X\",\"Y\")\ndf2 <- data.frame(mvrnorm(n = 50, mu = c(0,0), Sigma2))\nnames(df2)=c(\"X\",\"Y\")\n```\n:::\n\n\n::: {.column .fragment width=\"40%\"}\nFinding the covariance and correlation matrices:\n\n```{r}\n#| echo: true\ncov(df1) %>% round(3)\ncor(df1) %>% round(3)\n\ncov(df2) %>% round(3)\ncor(df2) %>% round(3)\n```\n:::\n\n## Correlation = covariance of scaled data!\n\n- It turns out that $R$ for a data set is just $\\Sigma$ if we scale the data first!\n\n::: column\n```{r}\n#| echo: true\n\n# Correlation of unscaled data:\ncor(df1) %>% round(3)\n\n#Scale the data:\ndf1_scaled <- scale(df1)\n\n#Covariance of scaled data:\ncov(df1_scaled) %>% round(3)\n```\n:::\n\n::: column\n```{r}\n#| echo: true\n\n# Correlation of unscaled data:\ncor(df2) %>% round(3)\n\n#Scale the data:\ndf2_scaled <- scale(df2)\n\n#Covariance of scaled data:\ncov(df2_scaled) %>% round(3)\n```\n:::\n\n## Implications for eigendecomposition\n\n- $\\Sigma$:\n  - Eigenvectors tell us the coordinates of the new axes in original space (or equivalently, how to rotate the data in original coordinates)\n  - Eigenvalues sum to total diagonals of $\\Sigma$ (total marginal variances)\n\n. . .\n\n- $R$:\n  - Eigenvectors tell us the coordinates of the new axes in scaled space (or equivalently, how to rotate the scaled data)\n  - Eigenvalues sum to total diagonals of $R$ \n  - **What will eigenvalues sum to if doing eigendecomposition of any $p\\times p$ correlation matrix?**\n\n\n## `df1`\n\n```{r}\n#| fig-width: 10\n#| fig-height: 5\n#| fig-align: center\n\necov  <- eigen(cov(df1))\necor  <- eigen(cor(df1))\necovvecs <- eigen(cov(df1))$vectors\necorvecs <- eigen(cor(df1))$vectors\n\nm1 <- sqrt(ecov$values[1])\nm2 <- sqrt(ecov$values[2])\n\nm1cor <- sqrt(ecor$values[1])\nm2cor <- sqrt(ecor$values[2])\n\n# Make plots\np1 <- ggplot() + geom_point(aes(X,Y),data = scale(df1, scale=FALSE), alpha=0.6) + \n  ggtitle(\"df1 original scale\") + theme_minimal(base_size = 14) + geom_hline(aes(yintercept = 0)) + geom_vline(aes(xintercept = 0)) + \n  geom_segment(aes(x =0, y=0, xend = m1*ecovvecs[1,1], yend  = m1*ecovvecs[2,1]),col='red',\n               arrow = arrow(length = unit(0.2, \"cm\")), linewidth = 1) + \n  geom_segment(aes(x =0, y=0, xend = m2*ecovvecs[1,2], yend  = m2*ecovvecs[2,2]),col='red',\n               arrow = arrow(length = unit(0.2, \"cm\")), linewidth = 1)\n\np2 <- ggplot() + geom_point(aes(X,Y),data = scale(df1), alpha=0.6) + \n  ggtitle(\"df1 scaled\") + theme_minimal(base_size = 14) + geom_hline(aes(yintercept = 0)) + geom_vline(aes(xintercept = 0)) + \n  geom_segment(aes(x =0, y=0, xend = m1cor*ecorvecs[1,1], yend  = m1cor*ecorvecs[2,1]),col='red',\n               arrow = arrow(length = unit(0.2, \"cm\")), linewidth = 1) + \n  geom_segment(aes(x =0, y=0, xend = m2cor*ecorvecs[1,2], yend  = m2cor*ecorvecs[2,2]),col='red',\n               arrow = arrow(length = unit(0.2, \"cm\")), linewidth = 1)\n```\n\n::: column \n\n```{r}\n#| fig-width: 3.5\n#| fig-asp: 1\n#| fig-align: center\np1\n```\n```{r}\n#| echo: true\neigen(cov(df1))\n```\n:::\n\n::: column \n\n```{r}\n#| fig-width: 3.5\n#| fig-asp: 1\n#| fig-align: center\np2\n```\n```{r}\n#| echo: true\neigen(cor(df1))\n```\n:::\n\n\n\n## `df2`\n\n```{r}\n#| fig-width: 10\n#| fig-height: 5\n#| fig-align: center\n\necov  <- eigen(cov(df2))\necor  <- eigen(cor(df2))\necovvecs <- eigen(cov(df2))$vectors\necorvecs <- eigen(cor(df2))$vectors\n\nm1 <- sqrt(ecov$values[1])\nm2 <- sqrt(ecov$values[2])\n\nm1cor <- sqrt(ecor$values[1])\nm2cor <- sqrt(ecor$values[2])\n\n# Make plots\np1 <- ggplot() + geom_point(aes(X,Y),data = scale(df2, scale=FALSE), alpha=0.6) + \n  ggtitle(\"df2 original scale\") + theme_minimal(base_size = 14) + geom_hline(aes(yintercept = 0)) + geom_vline(aes(xintercept = 0)) + \n  geom_segment(aes(x =0, y=0, xend = m1*ecovvecs[1,1], yend  = m1*ecovvecs[2,1]),col='red',\n               arrow = arrow(length = unit(0.2, \"cm\")), linewidth = 1) + \n  geom_segment(aes(x =0, y=0, xend = m2*ecovvecs[1,2], yend  = m2*ecovvecs[2,2]),col='red',\n               arrow = arrow(length = unit(0.2, \"cm\")), linewidth = 1)\n\np2 <- ggplot() + geom_point(aes(X,Y),data = scale(df2), alpha=0.6) + \n  ggtitle(\"df2 scaled\") + theme_minimal(base_size = 14) + geom_hline(aes(yintercept = 0)) + geom_vline(aes(xintercept = 0)) + \n  geom_segment(aes(x =0, y=0, xend = m1cor*ecorvecs[1,1], yend  = m1cor*ecorvecs[2,1]),col='red',\n               arrow = arrow(length = unit(0.2, \"cm\")), linewidth = 1) + \n  geom_segment(aes(x =0, y=0, xend = m2cor*ecorvecs[1,2], yend  = m2cor*ecorvecs[2,2]),col='red',\n               arrow = arrow(length = unit(0.2, \"cm\")), linewidth = 1)\n```\n\n::: column \n\n```{r}\n#| fig-width: 3.5\n#| fig-asp: 1\n#| fig-align: center\np1\n```\n```{r}\n#| echo: true\neigen(cov(df2))\n```\n:::\n\n::: column \n\n```{r}\n#| fig-width: 3.5\n#| fig-asp: 1\n#| fig-align: center\np2\n```\n```{r}\n#| echo: true\neigen(cor(df2))\n```\n:::\n\n","srcMarkdownNoYaml":"\n\n## Case study\n\nAs our case study, we're going to use some simulated data.\n\n```{r}\n#| echo: true\n#| warning: false\n#| message: false\n\nlibrary(MASS)\n\n# parameters\nmu <- c(4, 3)\nsig1 <- 1\nsig2 <- 3\nrho <- 0.8\nn <- 50\n\n# Covariance matrix\n\nSigma <- matrix(c(sig1^2, rho*sig1*sig2,\n                  rho*sig1*sig2, sig2^2), 2, 2)\n\n# Simulate n observations\nset.seed(555)  # for reproducibility\ndata <- mvrnorm(n=n, mu=mu, Sigma=Sigma)\n\n# Convert to data frame for plotting\ndf <- data.frame(data)\nnames(df) <- c('x','y')\nhead(df)\n```\n\n## Goal\n\nConsider plot of the data below:\n\n```{r}\n#| fig-align: center\n#| fig-width: 5\n#| fig-asp: 1\n#| warning: false\n#| message: false\n\nlibrary(patchwork)\nlibrary(tidyverse)\n\nlim <- c(-10,10)\n\nbase <- ggplot() +\n  scale_x_continuous(breaks=seq(min(lim), max(lim), by=1),limits=lim) +\n    scale_y_continuous(breaks=seq(min(lim), max(lim), by=1),limits=lim) + \n  geom_vline(aes(xintercept = 0)) +  geom_hline(aes(yintercept = 0)) +  \n  theme_minimal(base_size = 14) + \n  theme(panel.grid.minor = element_blank()) + \n  labs(x='', y='')\n\ndf_centered <- scale(df, scale = FALSE)\n\nevecs <- -eigen(cov(df))$vectors\nevals <- eigen(cov(df))$values\ndf_rotated <- data.frame(df_centered%*%evecs)\nnames(df_rotated) <- c('x','y')\n\n\np1 <- base+ geom_point(aes(x = x, y = y), data = df, size = 3, alpha = 0.8) + ggtitle('Data')\np2 <- base+ geom_point(aes(x = x, y = y), data = df_centered, size = 3, alpha = 0.8) + ggtitle('Center')\np3 <- base+ geom_point(aes(x = x, y = y), data = df_rotated, size = 3, alpha = 0.8) + ggtitle('Rotate')\n\np1\n```\n\nSuppose we want to:\n\n1.  Center the data on the origin;\n2.  Rotate it so that:\n\n-   it is no longer correlated;\n-   maximal variability is along the first (horizontal) dimension\n\n## Visual goal\n\n```{r}\n#| fig-width: 12\n#| fig-height: 4\np1+p2 +p3\n```\n\nThis is one of the fundamental ideas behind *principal component analysis*, which we'll cover in much more detail later on.\n\n## Defining new axes\n\nConceptually, the way we accomplish this is not by transforming the *points*, but by *defining new axes*:\n\n```{r}\n#| fig-width: 6\n#| fig-height: 6\n#| fig-align: center\nmeanvec <- apply(df, 2, mean)\n\nmult <- 5\nnewaxis <- p1 + \n  geom_segment(aes(x = meanvec[1], xend = meanvec[1]+mult*evecs[1,1],\n                   y = meanvec[2], yend = meanvec[2]+mult*evecs[2,1]),col='red') + \n    geom_segment(aes(x = meanvec[1], xend = meanvec[1]-mult*evecs[1,1],\n                   y = meanvec[2], yend = meanvec[2]-mult*evecs[2,1]),col='red') + \n    geom_segment(aes(x = meanvec[1], xend = meanvec[1]+mult*evecs[1,2],\n                   y = meanvec[2], yend = meanvec[2]+mult*evecs[2,2]),col='red') + \n    geom_segment(aes(x = meanvec[1], xend = meanvec[1]-mult*evecs[1,2],\n                   y = meanvec[2], yend = meanvec[2]-mult*evecs[2,2]),col='red') + \n  ggtitle('')\nnewaxis\n```\n\n... so how do we do this?\n\n## The mean vector\n\nThe first part is easy.\n\nThe coordinates of the new axis origin are simply the mean vector:\n\n```{r}\n#| echo: true\nmeanvec <- apply(df, 2, mean)\nmeanvec\n```\n\n```{r}\n#| fig-width: 6\n#| fig-height: 6\n#| fig-align: center\n\nnewaxis + \n  geom_point(aes(x =  meanvec[1], y =  meanvec[2]), size = 5, pch = 19, col='red') \n```\n\n## The covariance matrix\n\n-   The next ingredient is the *covariance matrix.*\\\n-   For $p=2$ dimensions, this measures:\n    -   Horizontal variability;\n    -   Vertical variability;\n    -   Covariance\n\n## Horizontal and vertical variance\n\nHorizontal and vertical variances are also known as ***marginal variances***:\n\n```{r}\n#| fig-width: 15\n#| fig-height: 5\nh <- base + geom_point(aes(x = x, y = rep(0,nrow(df))), size=3, data = df, alpha = 0.8) + ggtitle('Horizontal variability') + geom_point(aes(x = meanvec[1],y=0), size = 5, pch=19,col='red')\nv <- base + geom_point(aes(y = y, x = rep(0,nrow(df))), size =3,data = df, alpha = 0.8) + ggtitle('Vertical variability') + geom_point(aes(y = meanvec[2],x=0), size = 5, pch=19,col='red')\np1withmean <- p1 +   geom_point(aes(x =  meanvec[1], y =  meanvec[2]), size = 5, pch = 19, col='red') \n\np1withmean + h + v\n```\n\n. . .\n\nGiven mean-centered $n\\times 1$ column vectors $x_c$ and $y_c$:\n\n-   Horizontal marginal variability: $\\frac{1}{n-1}x_c^T x_c$\n-   Vertical marginal variability: $\\frac{1}{n-1}y_c^T y_c$\n\n## Computing horizontal and vertical variability {auto-animate=\"true\"}\n\n```{r}\n#| echo: true\n# Mean-center the columns:\ndf_centered <- scale(df, center = TRUE, scale = FALSE)\n```\n\n## Computing horizontal and vertical variability {auto-animate=\"true\"}\n\n```{r}\n#| echo: true\n# Mean-center the columns:\ndf_centered <- scale(df, center = TRUE, scale = FALSE)\n\n#Horizontal variability:\nx_c <- df_centered[,1]\nt(x_c) %*% x_c / (n-1)\n```\n\n## Computing horizontal and vertical variability {auto-animate=\"true\"}\n\n```{r}\n#| echo: true\n# Mean-center the columns:\ndf_centered <- scale(df, center = TRUE, scale = FALSE)\n\n#Horizontal variability:\nx_c <- df_centered[,1]\nt(x_c) %*% x_c / (n-1)\n\n#Vertical variability:\ny_c <- df_centered[,2]\nt(y_c) %*% y_c/ (n-1)\n```\n\nDoes this match our visual intuition?\n\n## Covariance\n\n::::: columns\n::: {.column width=\"50%\"}\n-   *Covariance* looks at the mean-centered data:\n\n```{r}\n#| fig-width: 6\n#| fig-height: 6\n#| fig-align: center\np2 + ggtitle('') \n```\n:::\n\n::: {.column .fragment width=\"50%\"}\n-   Then considers whether the $(x,y)$ pairs are above or below their means:\n\n```{r}\n#| fig-width: 6\n#| fig-height: 6\n#| fig-align: center\npos <- p2 + ggtitle('') + \n  geom_text(aes(x = 8, y=8, label = '(+, +)'), size =10)+\n  geom_text(aes(x = -8, y=8, label = '(-, +)'), size =10)+\n  geom_text(aes(x = -8, y=-8, label = '(-, -)'), size =10)+\n  geom_text(aes(x = 8, y=-8, label = '(+, -)'), size =10)\npos\n```\n:::\n:::::\n\n## Covariance\n\n-   Large $(+,+)$ and $(-,-)$ pairs contribute to large *positive* covariance\n-   Large $(+,-)$ and $(-,+)$ pairs contribute to large *negative* covariance\n\n```{r}\n#| fig-width: 15\n#| fig-height: 5\n#| fig-align: center\n\nrho2 <- -0.8\nrho3 <- 0\nSigma2 <- matrix(c(sig1^2, rho2*sig1*sig2,\n                  rho2*sig1*sig2, sig2^2), 2, 2)\nSigma3 <- matrix(c(sig1^2, rho3*sig1*sig2,\n                  rho3*sig1*sig2, sig2^2), 2, 2)\n\n# Simulate n observations\nset.seed(523)  \ndf2 <- data.frame(scale(mvrnorm(n=n, mu=mu, Sigma=Sigma2),scale=FALSE))\ndf3 <- data.frame(scale(mvrnorm(n=n, mu=mu, Sigma=Sigma3),scale=FALSE))\nnames(df2) <- names(df3) <- c('x','y')\n\nneg <- base+ geom_point(aes(x = x, y =y), data = df2, size = 3, alpha = 0.8)+\n  geom_text(aes(x = 8, y=8, label = '(+, +)'), size =10)+\n  geom_text(aes(x = -8, y=8, label = '(-, +)'), size =10)+\n  geom_text(aes(x = -8, y=-8, label = '(-, -)'), size =10)+\n  geom_text(aes(x = 8, y=-8, label = '(+, -)'), size =10) + \n  ggtitle(\"Lots of large (-,+) and (+,-) pairs\")\nzero <- base+ geom_point(aes(x = x, y =y), data = df3, size = 3, alpha = 0.8)+\n  geom_text(aes(x = 8, y=8, label = '(+, +)'), size =10)+\n  geom_text(aes(x = -8, y=8, label = '(-, +)'), size =10)+\n  geom_text(aes(x = -8, y=-8, label = '(-, -)'), size =10)+\n  geom_text(aes(x = 8, y=-8, label = '(+, -)'), size =10) + \n  ggtitle(\"(-,+) and (+,-) pairs balance the \n          (+,+) and (-,-) pairs\")\n\npostitle <- pos + \n  ggtitle(\"Lots of large (-,-) and (+,+) pairs\")\npostitle+neg+zero\n```\n\n## Computing covariance\n\nTo compute covariance: $\\frac{1}{n-1}x_c^T y_c$\n\n```{r}\n#| echo: true\nt(x_c)%*% y_c/ (n-1)\n```\n\n. . .\n\nAll ingredients (horizontal variance, vertical variance, and covariance) are often included in the *covariance matrix*, frequently notated by $\\Sigma$:\n\n$$ \\Sigma = \\begin{pmatrix} \\mbox{Horizontal variance} & \\mbox{Covariance} \\\\ \\mbox{Covariance} & \\mbox{Vertical variance} \\end{pmatrix}$$\n\n. . .\n\n```{r}\n#| echo: true\n#| attr-source: \"style='font-size: 2em'\"\n#| attr-output: \"style='font-size: 1em'\"\n\nSigma <- cov(df)\nSigma\n```\n\n## Covariance interpretation \n\nLet's take another look at this matrix:\n\n```{r}\n#| echo: true\n#| attr-source: \"style='font-size: 2em'\"\n#| attr-output: \"style='font-size: 1em'\"\nSigma\n```\n\n. . .\n\nSum of diagonal: ***total information in data*** (horizontal + vertical variance)\n\n```{r}\n#| echo: true\n#| attr-source: \"style='font-size: 2em'\"\n#| attr-output: \"style='font-size: 1.5em'\"\nsum(diag(Sigma))\n```\n\n. . .\n\nOff-diagonal: ***information in common between columns***\n```{r}\n#| attr-output: \"style='font-size: 1.5em'\"\nSigma[1,2]\n```\n\n## Quiz question\n\nMatch the covariance matrices to the scatterplot.\n\n![](images/clipboard-1080268104.png){fig-align=\"center\" width=\"400\"}\n\n![](images/clipboard-3029979869.png){width=\"500\" fig-align=\"center\"}\n\n\n\n## Eigen decomposition\n\n::: {.column}\n\n- We know where the new axis origin is:\n\n```{r .fragment}\n#| fig-width: 3\n#| fig-asp: 1\n#| fig-align: center\n\np1withmean + ggtitle('')\n```\n\n::: {.fragment}\n- We have the $2 \\times 2$ covariance matrix $\\Sigma$ that tells us about vertical variance, horizontal variance, and covariance:\n\n```{r}\nSigma\n```\n:::\n\n::: \n\n::: {.column .fragment}\n- What we need are the *vectors* that start from the new origin to form our new axis!\n\n\n```{r}\n#| fig-width: 4\n#| fig-asp: 1\n#| fig-align: center\n\nmult <- 2\neigenplot <- p1withmean + ggtitle('') + \n  geom_segment(aes(x = meanvec[1], \n                   xend = meanvec[1]+mult*evecs[1,1],\n                   y = meanvec[2], \n                   yend = meanvec[2] + mult*evecs[2,1]),col='red',\n               arrow = arrow(length = unit(0.1, \"cm\")), linewidth = 1) + \n    geom_segment(aes(x = meanvec[1], \n                     xend = meanvec[1]+mult*evecs[1,2],\n                   y = meanvec[2], \n                   yend = meanvec[2]+mult*evecs[2,2]),col='red',\n                 arrow = arrow(length = unit(0.1, \"cm\")), linewidth = 1)\n\neigenplot\n```\n:::\n\n## Eigenvector\n\n- Consider a $p\\times p$  matrix $A$.\n- A $p$-vector $v$ is an *eigenvector* of $A$ if, for a scalar $\\lambda$:\n\n$$A v = \\lambda v$$\n\n- In other words, eigenvectors are vectors that $A$ merely *shrinks* or *elongates*.  \n\n## Is it an eigenvector?\n\n- Consider the matrix $A = \\begin{pmatrix} 2 & 1 \\\\ 0 & 3 \\end{pmatrix}$.\n\n- Which of these are eigenvectors of $A$? \n\n$$ v_1 = \\begin{pmatrix} 1 \\\\0 \\end{pmatrix}; \\ \\ \\ \\ \nv_2 = \\begin{pmatrix} 0 \\\\1 \\end{pmatrix}; \\ \\ \\ \\ \nv_3 = \\begin{pmatrix} 2 \\\\2 \\end{pmatrix}$$\n$$v_4 = \\begin{pmatrix} 0.5 \\\\ 0.5 \\end{pmatrix};\\ \\ \\ \nv_5 = \\begin{pmatrix} 2 \\\\ 4 \\end{pmatrix};\\ \\ \\ \nv_6 = \\begin{pmatrix} 1/3 \\\\ 0 \\end{pmatrix}$$\n\n\n\n## Finding eigenvalues\n\n- There are infinitely many eigenvectors for a matrix $A$, but at most $p$ eigenvalues.\n- Thus, eigendecomposition starts with finding the candidate *eigenvalues*.  \n- An *eigenvalue* of $A$, a scalar notated $\\lambda$, satisfies the equation: \n\n\n$$det(A - \\lambda I) = 0$$ \n\nthis is sometimes referred to as the *characteristic polynomial*.  \n\n## Finding eigenvalues\n\n- Consider matrix $A = \\begin{pmatrix} 2 & 1 \\\\ 0 & 3 \\end{pmatrix}$.\n\n- Then:\n\n$$A  - \\lambda I = \\begin{pmatrix} 2 & 1 \\\\ 0 & 3 \\end{pmatrix}  - \\begin{pmatrix} \\lambda & 0 \\\\ 0& \\lambda \\end{pmatrix} $$\n\n\n. . .\n\n$$ =  \\begin{pmatrix} 2-\\lambda & 1 \\\\ 0 & 3-\\lambda \\end{pmatrix} $$\n\n. . .\n\n$$det(A-\\lambda I) = (2-\\lambda)(3-\\lambda) = 0$$\n\n. . . \n\n- Solutions: $\\lambda = 3$ or $\\lambda = 2$\n\n## Finding eigenvectors\n\n- Eigenvalues of $A = \\begin{pmatrix} 2 & 1 \\\\ 0 & 3 \\end{pmatrix}$ are $\\lambda = 2$ or $\\lambda = 3$.\n\n- Thus the *eigenvectors* of $A$ are vectors that $A$ either *doubles* or *triples* in magnitude!\n\n. . .\n\n- Reconsider the eigenvectors we identified earlier:\n\n\n$$ v_1 = \\begin{pmatrix} 1 \\\\0 \\end{pmatrix}; \\ \\ \\ \\ \nv_3 = \\begin{pmatrix} 2 \\\\2 \\end{pmatrix}; \\ \\ \\ \\ \nv_4 = \\begin{pmatrix} 0.5 \\\\ 0.5 \\end{pmatrix};\\ \\ \\ \nv_6 = \\begin{pmatrix} 1/3 \\\\ 0 \\end{pmatrix}$$\n\nWhich ones does $A$ double?  Which ones does $A$ triple? \n\n\n## Finding eigenvectors for $\\lambda = 3$ \n\nTo find eigenvectors for $\\lambda = 3$ (ones that $A$ triples), recall eigenvectors satisfy:\n\n$$Av = \\lambda v = 3v$$\n\n. . .\n\nIn matrix form: \n\n$$\\begin{pmatrix} 2 & 1 \\\\ 0 & 3 \\end{pmatrix}\\begin{pmatrix} x \\\\ y\\end{pmatrix} = 3 \\begin{pmatrix} x\\\\ y \\end{pmatrix}$$\n\n. . .\n\n$$ \\begin{pmatrix} 2x+y  \\\\ 0x + 3y \\end{pmatrix} = \\begin{pmatrix}3x \\\\ 3y \\end{pmatrix}$$\n\n. . . \n\n$$x = 1; y = 1$$\n\n. . .\n\nThus $v = \\begin{pmatrix} 1\\\\ 1 \\end{pmatrix},$ or any multiple of $v$, is an eigenvector that $A$ triples!\n\n\n\n## Finding eigenvectors for $\\lambda = 2$ \n\nTo find eigenvectors for $\\lambda = 2$ (ones that $A$ doubles), recall eigenvectors satisfy:\n\n$$Av = \\lambda v = 2v$$\n\n. . .\n\nIn matrix form: \n\n$$\\begin{pmatrix} 2 & 1 \\\\ 0 & 3 \\end{pmatrix}\\begin{pmatrix} x \\\\ y\\end{pmatrix} = 2 \\begin{pmatrix} x\\\\ y \\end{pmatrix}$$\n\n. . .\n\n$$ \\begin{pmatrix} 2x+y  \\\\ 0x + 3y \\end{pmatrix} = \\begin{pmatrix}2x \\\\ 2y \\end{pmatrix}$$\n\n. . . \n\n$$x = 1; y = 0$$\n\n. . .\n\nThus $v = \\begin{pmatrix} 1\\\\ 0 \\end{pmatrix},$ or any multiple of $v$, is an eigenvector that $A$ doubles!\n\n\n## Eigenvectors of symmetric matrices are orthogonal !\n\n- If we are decomposing a *symmetric* $p\\times p$ matrix $B$, then eigenvectors for different eigenvalues are orthogonal!\n- Recall that orthogonal (i.e., perpendicular) vectors have dot product = 0\n\n. . .\n\n- Recall eigenvectors for $A =\\begin{pmatrix} 2 & 1 \\\\ 0 & 3 \\end{pmatrix}$:  $v_1=\\begin{pmatrix} 1\\\\ 0 \\end{pmatrix}$ and $v_2 =  \\begin{pmatrix} 1\\\\ 1 \\end{pmatrix}$.  Are they orthogonal?  Why/why not?\n\n. . .\n\n- The orthogonality of eigenvectors of symmetric matrices is important for decomposing covariance matrices! (coming up)\n\n## Eigenbases\n\n- For $\\lambda = 3$, $\\begin{pmatrix} 1  \\\\ 1 \\end{pmatrix}$ *or any multiple thereof* is an eigenvector.\n\n- For $\\lambda = 2$, $\\begin{pmatrix} 1  \\\\ 0 \\end{pmatrix}$ *or any multiple thereof* is an eigenvector.\n\n- Thus for any eigenvalue there are infinitely many eigenvectors.\n\n- *Eigenbases* are unique, normed (length-1) vectors for each eigenvalue\n\n$$v_{norm} = \\frac{v}{||v||_2}$$\n\n\n## Eigenbase for $\\lambda = 3$\n\nL2-normed  $\\lambda = 3$ eigenvector $\\begin{pmatrix} 1  \\\\ 1 \\end{pmatrix}$:\n\n$$ \\sqrt{1^2 + 1^2} = \\sqrt{2}$$\n\nNormed $\\lambda = 3$ eigenvector: \n\n$$\\begin{pmatrix} 1/\\sqrt{2}  \\\\ 1/\\sqrt{2} \\end{pmatrix} =\\begin{pmatrix} 0.7071068  \\\\ 0.7071068 \\end{pmatrix} $$\n\n\n## Eigenbase for $\\lambda = 2$\n\nL2-normed  $\\lambda = 2$ eigenvector $\\begin{pmatrix} 1  \\\\ 0 \\end{pmatrix}$:\n\n$$ \\sqrt{1^2 + 0^2} = \\sqrt{1}$$\n\nSo $\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ already has L2 norm = 1! \n\n## `eigen` in `R`\n\nTo find eigenvalues and eigenbases in `R`:\n\n```{r}\n#| echo: true\n#| attr-source: \"style='font-size: 2em'\"\n#| attr-output: \"style='font-size: 1em'\"\n\nA <- matrix(c(2,1,\n              0,3),\n            2,2, byrow=TRUE)\neigen(A)\n```\n\n## Tying it all together\n\n- So we have this covariance matrix (which, by the way, is symmetric, as all covariance matrices will be):\n\n\n```{r}\n#| echo: true\nSigma\n```\n\n. . .\n\n-  The *eigenbases* of $\\Sigma$ are the new axes we want!!\n- And, since $\\Sigma$ is symmetric, they are guaranteed orthogonal (so will work as new axes).\n\n```{r}\n#| echo: true\neig_decomposition <- eigen(Sigma)\neig_decomposition\n```\n\n## Plotting the bases\n\n```{r}\n#| echo: true\n#| fig-width: 6\n#| fig-asp: 1\n#| fig-align: center\neigenvectors <- eig_decomposition$vectors\n\nvectordf <- data.frame(t(eigenvectors)) %>% \n  mutate(mean_x = meanvec[1], mean_y = meanvec[2]) %>% \n  rename(evector_xdirection = X1, evector_ydirection = X2)\n\nvectordf\neigenbaseplot <- base + \n  geom_point(aes(x = x, y = y), data = df, alpha = 0.8, size = 3) + \n  geom_point(aes(x = mean_x,  y = mean_y), data = vectordf, color='red',size = 4) + \n  geom_segment(aes(x = mean_x, y = mean_y, \n                   xend = mean_x +evector_xdirection, \n                   yend = mean_y + evector_ydirection), \n               color='red', data = vectordf,  \n               arrow = arrow(length = unit(0.2, \"cm\")), linewidth = 1) \neigenbaseplot\n```\n\n## Scaling the bases\n\nIn this plot we see the *directions* of the axes:\n\n```{r}\n#| fig-width: 6\n#| fig-asp: 1\n#| fig-align: center\n#| \neigenbaseplot\n```\n\n. . .\n\nThe eigenvalues give us a sense as to how much variability is in the *direction* of each axis:\n\n```{r}\neigenvalues <- eig_decomposition$values\neigenvalues\n```\n\n. . .\n\nThese values indicate there is a lot more variability in the $e_1$ (first eigenvector) direction than in the $e_2$ (second eigenvector) direction.\n\n## Scaling the bases\n\nWe can scale the axes accordingly by the square root of the eigenvalues:\n\n::: column\n```{r}\n#| echo: true\nvectordf <- data.frame(t(eigenvectors)) %>% \n  mutate(mean_x = meanvec[1], mean_y = meanvec[2]) %>% \n  rename(evector_xdirection = X1, evector_ydirection = X2) %>% \n  mutate(evector_xdirection_scaled = sqrt(eigenvalues)*evector_xdirection,\n         evector_ydirection_scaled = sqrt(eigenvalues)*evector_ydirection)\n\nvectordf\n```\n:::\n\n::: column\n```{r}\n#| fig-width: 6\n#| fig-asp: 1\n#| fig-align: center\nbase + \n  geom_point(aes(x = x, y = y), data = df, alpha = 0.8, size = 2) + \n  geom_point(aes(x = mean_x,  y = mean_y), data = vectordf, color='red',size = 2) + \n  geom_segment(aes(x = mean_x, y = mean_y, \n                   xend = mean_x +evector_xdirection_scaled, \n                   yend = mean_y + evector_ydirection_scaled), \n               color='red', data = vectordf,  \n               arrow = arrow(length = unit(0.2, \"cm\")), linewidth = 1) \n```\n:::\n\n## Interpreting the eigenvalues\n\n- The eigenvalues also have meaningful interpretations in terms of variability. \n\n- Recall:\n\n```{r}\n#| echo: true\nSigma\nsum(diag(Sigma))\n```\n\n... so the total horizontal and vertical variance is `r sum(diag(Sigma))`.\n\n. . .\n\nNow consider the eigenvalues:\n\n```{r}\n#| echo: true\neigenvalues\nsum(eigenvalues)\n```\n\n... so the summed eigenvalues *also* equal the total horizontal and vertical variance!!\n\n## Eigenvectors as rotation tools\n\n- Reconsider our mean-centered data:\n\n```{r}\n#| echo: true\ndf_centered <- scale(df, center = TRUE, scale = FALSE)\n```\n\n```{r}\n#| fig-width: 6\n#| fig-asp: 1\n#| fig-align: center\np2 + ggtitle('')\n```\n\n\n## Eigenvectors as rotation tools\n\n- Multiplying the centered data by the eigenvectors yields the *rotation* that treats the eigenvectors as the new horizontal and vertical axes:\n\n```{r}\n#| echo: true\ndf_rotated <- data.frame(df_centered %*% eigenvectors)\nnames(df_rotated) <- c('x','y')\n```\n\n```{r}\n#| fig-width: 6\n#| fig-asp: 1\n#| fig-align: center\np3 + ggtitle('')\n```\n\n## Eigenvalues as new covariance diagonals\n\n- Let's look at the covariance matrix of the centered, rotated data:\n\n```{r}\n#| echo: true\ncov(df_rotated) %>% round(4)\n```\n\n. . .\n\n- Hopefully these look familiar:\n\n```{r}\n#| echo: true\neigenvalues\n```\n\n- Thus the eigenvalues tell us the total horizontal and vertical variability of the mean centered data rotated to orient along the eigenvectors!\n\n## Summary\n\n- Covariance matrix $\\Sigma$: symmetric $p\\times p$ matrix\n  - Marginal variance of each dimension on the diagonal\n  - Covariance on the off-diagonals\n\n. . .\n\n- Eigenvectors of a square $p\\times p$ matrix $A$\n  - Vectors that $A$ scales by a constant\n  - Can be normalized to determine *eigenbases*\n  - If $A$ is symmetric, eigenvectors of different eigenvalues are orthogonal\n\n. . .\n\n- Eigenvalues\n  - Scalars that tell us how much eigenvectors are scaled by $A$ \n\n## Summary\n\nEigendecomposition of $\\Sigma$: \n\n- Eigenvectors\n  - 1) tell us which directions the reoriented axes go;\n  - 2) give us rules for rotating the data to be uncorrelated and reoriented;\n  - 3) are orthogonal (since $\\Sigma$ is symmetric)\n- Eigenvalues tell us the values of marginal horizontal and vertical variance after rotation\n  \n## Correlation vs covariance\n\n- Consider the following two covariance matrices from two data sets, `df1` and `df2`.\n- In both, we know the covariance but not the marginal (horizontal/vertical) variances:\n\n$$ \\Sigma_1 = \\begin{pmatrix}{? & 100 \\\\ 100 & ?}\\end{pmatrix}\\ \\ \\  \\Sigma_2 = \\begin{pmatrix}{? & 0.1 \\\\ 0.1 & ?}\\end{pmatrix}$$\n\n- Which matrix represents a data with more relationship between the two columns?\n\n\n## Correlation vs covariance\n\n- Consider the following two covariance matrices from two data sets, `df1` and `df2`.\n- In both, we know the covariance but not the marginal (horizontal/vertical) variances:\n\n$$ \\Sigma_1 = \\begin{pmatrix}? & 50 \\\\ 50 & ?\\end{pmatrix};\\ \\ \\  \\Sigma_2 = \\begin{pmatrix}? & 0.1 \\\\ 0.1 & ?\\end{pmatrix}$$\n\n- Which matrix represents a dataset with more relationship between the two columns?\n\n## Plotting the two data sets\n\n```{r}\n#| fig-width: 10\n#| fig-height: 5\n#| fig-align: center\n\n\nmu <- c(0,0)\nsigx <- 10; sigy <- 50; rho <- 0.1\nSigma1 <- matrix(c(sigx^2, sigx*sigy*rho,sigx*sigy*rho,sigy^2), nrow=2)\nsigx <- 1/3; sigy <- 1/3; rho <- .9\nSigma2 <- matrix(c(sigx^2, sigx*sigy*rho,sigx*sigy*rho,sigy^2), nrow=2)\n\n# Simulate data\nset.seed(944)\ndf1 <- as.data.frame(mvrnorm(n = 50, mu = c(0,0), Sigma1)); names(df1)=c(\"X\",\"Y\")\ndf2 <- as.data.frame(mvrnorm(n = 50, mu = c(0,0), Sigma2)); names(df2)=c(\"X\",\"Y\")\n\n# Make plots\np1 <- ggplot(df1, aes(X,Y)) + geom_point(alpha=0.6) + ggtitle(\"Plot of df1\") + theme_minimal(base_size = 14) + geom_hline(aes(yintercept = 0)) + geom_vline(aes(xintercept = 0))\np2 <- ggplot(df2, aes(X,Y)) + geom_point(alpha=0.6) + ggtitle(\"Plot of df2\") + theme_minimal(base_size = 14)+  geom_hline(aes(yintercept = 0)) + geom_vline(aes(xintercept = 0))\n\n\n# Arrange side by side\np1+p2\n```\n- Which data set has more relationship?  \n- What differences do you notice about between these two data sets? \n\n## Covariance vs correlation\n\n- Let $\\sigma_1^2$ represent the marginal variance in the first dimension (e.g., horizontal)\n- Let $\\sigma_2^2$ represent the marginal variance in the second dimension (e.g., vertical)\n- The *correlation*, $\\rho$, between two variables $x$ and $y$ is defined as:\n\n$$\\rho = \\frac{Cov(x,y)}{\\sigma_1 \\sigma_2}$$\n\n. . .\n\n- Note that this also implies that $Cov(x,y) = \\rho \\sigma_1 \\sigma_2$; in other words, that the covariance is a function both of the *relationship* between variables and the *marginal* variances!\n\n## Facts about $\\rho$\n\n- Unitless\n- $-1 \\leq \\rho \\leq 1$\n- $|\\rho|$ close to 1 $\\rightarrow$ strong relationship\n- $|\\rho|$ close to 0 $\\rightarrow$ weak relationship\n\n## Covariance - correlation relationship\n\n::: {.column width=\"40%\"}\nFor 2D data:\n\n$$\\Sigma = \\begin{pmatrix}\\sigma_1^2 & \\sigma_1 \\sigma_2 \\rho\\\\\n\\sigma_1 \\sigma_2 \\rho & \\sigma_2^2 \\end{pmatrix}$$\n\n$$R = \\begin{pmatrix}1 &  \\rho\\\\\n\\rho & 1 \\end{pmatrix}$$\n:::\n\n::: {.column .fragment width=\"60%\"}\nFor $p$-dimensional data:\n\n$$\\Sigma = \\begin{pmatrix}\\sigma_1^2 & \\sigma_1 \\sigma_2 \\rho_{12} & \\sigma_1 \\sigma_3 \\rho_{13} & ... & \\sigma_1 \\sigma_p \\rho_{1p}\\\\\n\\sigma_1 \\sigma_2 \\rho_{12} & \\sigma_2^2 & \\sigma_2 \\sigma_3 \\rho_{23} & ... & \\sigma_2 \\sigma_p \\rho_{2p}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\ \n\\sigma_1 \\sigma_p \\rho_{1p} & \\sigma_2\\sigma_p \\rho_{2p} & \\sigma_3 \\sigma_p \\rho_{3p} & ... & \\sigma_p^2\\end{pmatrix}$$\n\n\n$$R = \\begin{pmatrix}1 &  \\rho_{12} & \\rho_{13} & ... & \\rho_{1p}\\\\\n\\rho_{12} & 1 & \\rho_{23} & ... & \\rho_{2p}\\\\ \n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n\\rho_{1p} & \\rho_{2p} & \\rho_{3p} & ... & 1\\end{pmatrix}$$\n:::\n\n## Find the correlation\n\nThe two covariance matrices I used to simulate `df1` and `df2` were:\n\n\n$$ \\Sigma_1 = \\begin{pmatrix}100 & 50 \\\\ 50 & 2500\\end{pmatrix};\\ \\ \\  \\Sigma_2 = \\begin{pmatrix}1/9 & 0.1 \\\\ 0.1 & 1/9\\end{pmatrix}$$\n\n- What is $\\rho_1$, the correlation between the two variables for `df1`?\n- What is $\\rho_2$, the correlation between the two variables for `df2`?\n\n## Simulated data sets\n\n\n::: {.column width=\"60%\"}\nCode I used for simulating the two data sets:\n```{r}\n#| echo: true\n\n#Parameters:\nsigx <- 10; sigy <- 50; rho <- 0.1\nSigma1 <- matrix(c(sigx^2, sigx*sigy*rho,\n                   sigx*sigy*rho,sigy^2), nrow=2)\n\nsigx <- 1/3; sigy <- 1/3; rho <- .9\nSigma2 <- matrix(c(sigx^2, sigx*sigy*rho,\n                   sigx*sigy*rho,sigy^2), nrow=2)\n\n# Simulate data\nlibrary(MASS)\nset.seed(944)\ndf1 <- data.frame(mvrnorm(n = 50, mu = c(0,0), Sigma1))\nnames(df1)=c(\"X\",\"Y\")\ndf2 <- data.frame(mvrnorm(n = 50, mu = c(0,0), Sigma2))\nnames(df2)=c(\"X\",\"Y\")\n```\n:::\n\n\n::: {.column .fragment width=\"40%\"}\nFinding the covariance and correlation matrices:\n\n```{r}\n#| echo: true\ncov(df1) %>% round(3)\ncor(df1) %>% round(3)\n\ncov(df2) %>% round(3)\ncor(df2) %>% round(3)\n```\n:::\n\n## Correlation = covariance of scaled data!\n\n- It turns out that $R$ for a data set is just $\\Sigma$ if we scale the data first!\n\n::: column\n```{r}\n#| echo: true\n\n# Correlation of unscaled data:\ncor(df1) %>% round(3)\n\n#Scale the data:\ndf1_scaled <- scale(df1)\n\n#Covariance of scaled data:\ncov(df1_scaled) %>% round(3)\n```\n:::\n\n::: column\n```{r}\n#| echo: true\n\n# Correlation of unscaled data:\ncor(df2) %>% round(3)\n\n#Scale the data:\ndf2_scaled <- scale(df2)\n\n#Covariance of scaled data:\ncov(df2_scaled) %>% round(3)\n```\n:::\n\n## Implications for eigendecomposition\n\n- $\\Sigma$:\n  - Eigenvectors tell us the coordinates of the new axes in original space (or equivalently, how to rotate the data in original coordinates)\n  - Eigenvalues sum to total diagonals of $\\Sigma$ (total marginal variances)\n\n. . .\n\n- $R$:\n  - Eigenvectors tell us the coordinates of the new axes in scaled space (or equivalently, how to rotate the scaled data)\n  - Eigenvalues sum to total diagonals of $R$ \n  - **What will eigenvalues sum to if doing eigendecomposition of any $p\\times p$ correlation matrix?**\n\n\n## `df1`\n\n```{r}\n#| fig-width: 10\n#| fig-height: 5\n#| fig-align: center\n\necov  <- eigen(cov(df1))\necor  <- eigen(cor(df1))\necovvecs <- eigen(cov(df1))$vectors\necorvecs <- eigen(cor(df1))$vectors\n\nm1 <- sqrt(ecov$values[1])\nm2 <- sqrt(ecov$values[2])\n\nm1cor <- sqrt(ecor$values[1])\nm2cor <- sqrt(ecor$values[2])\n\n# Make plots\np1 <- ggplot() + geom_point(aes(X,Y),data = scale(df1, scale=FALSE), alpha=0.6) + \n  ggtitle(\"df1 original scale\") + theme_minimal(base_size = 14) + geom_hline(aes(yintercept = 0)) + geom_vline(aes(xintercept = 0)) + \n  geom_segment(aes(x =0, y=0, xend = m1*ecovvecs[1,1], yend  = m1*ecovvecs[2,1]),col='red',\n               arrow = arrow(length = unit(0.2, \"cm\")), linewidth = 1) + \n  geom_segment(aes(x =0, y=0, xend = m2*ecovvecs[1,2], yend  = m2*ecovvecs[2,2]),col='red',\n               arrow = arrow(length = unit(0.2, \"cm\")), linewidth = 1)\n\np2 <- ggplot() + geom_point(aes(X,Y),data = scale(df1), alpha=0.6) + \n  ggtitle(\"df1 scaled\") + theme_minimal(base_size = 14) + geom_hline(aes(yintercept = 0)) + geom_vline(aes(xintercept = 0)) + \n  geom_segment(aes(x =0, y=0, xend = m1cor*ecorvecs[1,1], yend  = m1cor*ecorvecs[2,1]),col='red',\n               arrow = arrow(length = unit(0.2, \"cm\")), linewidth = 1) + \n  geom_segment(aes(x =0, y=0, xend = m2cor*ecorvecs[1,2], yend  = m2cor*ecorvecs[2,2]),col='red',\n               arrow = arrow(length = unit(0.2, \"cm\")), linewidth = 1)\n```\n\n::: column \n\n```{r}\n#| fig-width: 3.5\n#| fig-asp: 1\n#| fig-align: center\np1\n```\n```{r}\n#| echo: true\neigen(cov(df1))\n```\n:::\n\n::: column \n\n```{r}\n#| fig-width: 3.5\n#| fig-asp: 1\n#| fig-align: center\np2\n```\n```{r}\n#| echo: true\neigen(cor(df1))\n```\n:::\n\n\n\n## `df2`\n\n```{r}\n#| fig-width: 10\n#| fig-height: 5\n#| fig-align: center\n\necov  <- eigen(cov(df2))\necor  <- eigen(cor(df2))\necovvecs <- eigen(cov(df2))$vectors\necorvecs <- eigen(cor(df2))$vectors\n\nm1 <- sqrt(ecov$values[1])\nm2 <- sqrt(ecov$values[2])\n\nm1cor <- sqrt(ecor$values[1])\nm2cor <- sqrt(ecor$values[2])\n\n# Make plots\np1 <- ggplot() + geom_point(aes(X,Y),data = scale(df2, scale=FALSE), alpha=0.6) + \n  ggtitle(\"df2 original scale\") + theme_minimal(base_size = 14) + geom_hline(aes(yintercept = 0)) + geom_vline(aes(xintercept = 0)) + \n  geom_segment(aes(x =0, y=0, xend = m1*ecovvecs[1,1], yend  = m1*ecovvecs[2,1]),col='red',\n               arrow = arrow(length = unit(0.2, \"cm\")), linewidth = 1) + \n  geom_segment(aes(x =0, y=0, xend = m2*ecovvecs[1,2], yend  = m2*ecovvecs[2,2]),col='red',\n               arrow = arrow(length = unit(0.2, \"cm\")), linewidth = 1)\n\np2 <- ggplot() + geom_point(aes(X,Y),data = scale(df2), alpha=0.6) + \n  ggtitle(\"df2 scaled\") + theme_minimal(base_size = 14) + geom_hline(aes(yintercept = 0)) + geom_vline(aes(xintercept = 0)) + \n  geom_segment(aes(x =0, y=0, xend = m1cor*ecorvecs[1,1], yend  = m1cor*ecorvecs[2,1]),col='red',\n               arrow = arrow(length = unit(0.2, \"cm\")), linewidth = 1) + \n  geom_segment(aes(x =0, y=0, xend = m2cor*ecorvecs[1,2], yend  = m2cor*ecorvecs[2,2]),col='red',\n               arrow = arrow(length = unit(0.2, \"cm\")), linewidth = 1)\n```\n\n::: column \n\n```{r}\n#| fig-width: 3.5\n#| fig-asp: 1\n#| fig-align: center\np1\n```\n```{r}\n#| echo: true\neigen(cov(df2))\n```\n:::\n\n::: column \n\n```{r}\n#| fig-width: 3.5\n#| fig-asp: 1\n#| fig-align: center\np2\n```\n```{r}\n#| echo: true\neigen(cor(df2))\n```\n:::\n\n"},"formats":{"revealjs":{"identifier":{"display-name":"RevealJS","target-format":"revealjs","base-format":"revealjs"},"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":{"method":"mathjax","url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js?config=TeX-AMS_HTML-full"},"slide-level":2,"to":"revealjs","output-file":"3 - covariance and eigendecomposition.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.7.32","auto-stretch":true,"editor":"visual","title":"1.3 - covariance and eigendecomposition","editor_options":{"chunk_output_type":"console"},"slideNumber":true,"smaller":true}}},"projectFormats":[]}