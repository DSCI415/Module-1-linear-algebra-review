{
  "hash": "b567a302094935f159b6e11675149c43",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"1.3 - covariance and eigendecomposition\"\nformat: \n  revealjs:\n    slide-number: true\n    smaller: true\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n## Case study\n\nAs our case study, we're going to use some simulated data.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(MASS)\n\n# parameters\nmu <- c(4, 3)\nsig1 <- 1\nsig2 <- 3\nrho <- 0.8\nn <- 50\n\n# Covariance matrix\n\nSigma <- matrix(c(sig1^2, rho*sig1*sig2,\n                  rho*sig1*sig2, sig2^2), 2, 2)\n\n# Simulate n observations\nset.seed(555)  # for reproducibility\ndata <- mvrnorm(n=n, mu=mu, Sigma=Sigma)\n\n# Convert to data frame for plotting\ndf <- data.frame(data)\nnames(df) <- c('x','y')\nhead(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         x         y\n1 3.085552  2.189201\n2 4.368236  4.522703\n3 4.151895  4.165595\n4 5.690867  8.624101\n5 2.615748 -2.358321\n6 5.267545  5.505842\n```\n\n\n:::\n:::\n\n\n\n## Goal\n\nConsider plot of the data below:\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](1.3---covariance-and-eigendecomposition_files/figure-revealjs/unnamed-chunk-2-1.png){fig-align='center' width=480}\n:::\n:::\n\n\n\nSuppose we want to:\n\n1.  Center the data on the origin;\n2.  Rotate it so that:\n\n-   it is no longer correlated;\n-   maximal variability is along the first (horizontal) dimension\n\n## Visual goal\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](1.3---covariance-and-eigendecomposition_files/figure-revealjs/unnamed-chunk-3-1.png){width=1152}\n:::\n:::\n\n\n\nThis is one of the fundamental ideas behind *principal component analysis*, which we'll cover in much more detail later on.\n\n## Defining new axes\n\nConceptually, the way we accomplish this is not by transforming the *points*, but by *defining new axes*:\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](1.3---covariance-and-eigendecomposition_files/figure-revealjs/unnamed-chunk-4-1.png){fig-align='center' width=576}\n:::\n:::\n\n\n\n... so how do we do this?\n\n## The mean vector\n\nThe first part is easy.\n\nThe coordinates of the new axis origin are simply the mean vector:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmeanvec <- apply(df, 2, mean)\nmeanvec\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       x        y \n4.004742 3.075866 \n```\n\n\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](1.3---covariance-and-eigendecomposition_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=576}\n:::\n:::\n\n\n\n## The covariance matrix\n\n-   The next ingredient is the *covariance matrix.*\\\n-   For $p=2$ dimensions, this measures:\n    -   Horizontal variability;\n    -   Vertical variability;\n    -   Covariance\n\n## Horizontal and vertical variance\n\nHorizontal and vertical variances are also known as ***marginal variances***:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](1.3---covariance-and-eigendecomposition_files/figure-revealjs/unnamed-chunk-7-1.png){width=1440}\n:::\n:::\n\n\n\n. . .\n\nGiven mean-centered $n\\times 1$ column vectors $x_c$ and $y_c$:\n\n-   Horizontal marginal variability: $\\frac{1}{n-1}x_c^T x_c$\n-   Vertical marginal variability: $\\frac{1}{n-1}y_c^T y_c$\n\n## Computing horizontal and vertical variability {auto-animate=\"true\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Mean-center the columns:\ndf_centered <- scale(df, center = TRUE, scale = FALSE)\n```\n:::\n\n\n\n## Computing horizontal and vertical variability {auto-animate=\"true\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Mean-center the columns:\ndf_centered <- scale(df, center = TRUE, scale = FALSE)\n\n#Horizontal variability:\nx_c <- df_centered[,1]\nt(x_c) %*% x_c / (n-1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         [,1]\n[1,] 1.276887\n```\n\n\n:::\n:::\n\n\n\n## Computing horizontal and vertical variability {auto-animate=\"true\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Mean-center the columns:\ndf_centered <- scale(df, center = TRUE, scale = FALSE)\n\n#Horizontal variability:\nx_c <- df_centered[,1]\nt(x_c) %*% x_c / (n-1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         [,1]\n[1,] 1.276887\n```\n\n\n:::\n\n```{.r .cell-code}\n#Vertical variability:\ny_c <- df_centered[,2]\nt(y_c) %*% y_c/ (n-1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         [,1]\n[1,] 8.774893\n```\n\n\n:::\n:::\n\n\n\nDoes this match our visual intuition?\n\n## Covariance\n\n::::: columns\n::: {.column width=\"50%\"}\n-   *Covariance* looks at the mean-centered data:\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](1.3---covariance-and-eigendecomposition_files/figure-revealjs/unnamed-chunk-11-1.png){fig-align='center' width=576}\n:::\n:::\n\n\n:::\n\n::: {.column .fragment width=\"50%\"}\n-   Then considers whether the $(x,y)$ pairs are above or below their means:\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](1.3---covariance-and-eigendecomposition_files/figure-revealjs/unnamed-chunk-12-1.png){fig-align='center' width=576}\n:::\n:::\n\n\n:::\n:::::\n\n## Covariance\n\n-   Large $(+,+)$ and $(-,-)$ pairs contribute to large *positive* covariance\n-   Large $(+,-)$ and $(-,+)$ pairs contribute to large *negative* covariance\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](1.3---covariance-and-eigendecomposition_files/figure-revealjs/unnamed-chunk-13-1.png){fig-align='center' width=1440}\n:::\n:::\n\n\n\n## Computing covariance\n\nTo compute covariance: $\\frac{1}{n-1}x_c^T y_c$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt(x_c)%*% y_c/ (n-1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         [,1]\n[1,] 2.946598\n```\n\n\n:::\n:::\n\n\n\n. . .\n\nAll ingredients (horizontal variance, vertical variance, and covariance) are often included in the *covariance matrix*, frequently notated by $\\Sigma$:\n\n$$ \\Sigma = \\begin{pmatrix} \\mbox{Horizontal variance} & \\mbox{Covariance} \\\\ \\mbox{Covariance} & \\mbox{Vertical variance} \\end{pmatrix}$$\n\n. . .\n\n\n\n::: {.cell}\n\n```{.r .cell-code style='font-size: 2em'}\nSigma <- cov(df)\nSigma\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```{style='font-size: 1em'}\n         x        y\nx 1.276887 2.946598\ny 2.946598 8.774893\n```\n\n\n:::\n:::\n\n\n\n## Covariance interpretation\n\nLet's take another look at this matrix:\n\n\n\n::: {.cell}\n\n```{.r .cell-code style='font-size: 2em'}\nSigma\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```{style='font-size: 1em'}\n         x        y\nx 1.276887 2.946598\ny 2.946598 8.774893\n```\n\n\n:::\n:::\n\n\n\n. . .\n\nSum of diagonal: ***total information in data*** (horizontal + vertical variance)\n\n\n\n::: {.cell}\n\n```{.r .cell-code style='font-size: 2em'}\nsum(diag(Sigma))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```{style='font-size: 1.5em'}\n[1] 10.05178\n```\n\n\n:::\n:::\n\n\n\n. . .\n\nOff-diagonal: ***information in common between columns***\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```{style='font-size: 1.5em'}\n[1] 2.946598\n```\n\n\n:::\n:::\n\n\n\n## Quiz question\n\nMatch the covariance matrices to the scatterplot.\n\n![](images/clipboard-1080268104.png){fig-align=\"center\" width=\"400\"}\n\n![](images/clipboard-3029979869.png){width=\"500\" fig-align=\"center\"}\n\n## Eigen decomposition\n\n:::: column\n-   We know where the new axis origin is:\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](1.3---covariance-and-eigendecomposition_files/figure-revealjs/.fragment-1.png){fig-align='center' width=288}\n:::\n:::\n\n\n\n::: fragment\n-   We have the $2 \\times 2$ covariance matrix $\\Sigma$ that tells us about vertical variance, horizontal variance, and covariance:\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n         x        y\nx 1.276887 2.946598\ny 2.946598 8.774893\n```\n\n\n:::\n:::\n\n\n:::\n::::\n\n::: {.column .fragment}\n-   What we need are the *vectors* that start from the new origin to form our new axis!\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](1.3---covariance-and-eigendecomposition_files/figure-revealjs/unnamed-chunk-20-1.png){fig-align='center' width=384}\n:::\n:::\n\n\n:::\n\n## Eigenvector\n\n-   Consider a $p\\times p$ matrix $A$.\n-   A $p$-vector $v$ is an *eigenvector* of $A$ if, for a scalar $\\lambda$:\n\n$$A v = \\lambda v$$\n\n-   In other words, eigenvectors are vectors that $A$ merely *shrinks* or *elongates*.\n\n## Is it an eigenvector?\n\n-   Consider the matrix $A = \\begin{pmatrix} 2 & 1 \\\\ 0 & 3 \\end{pmatrix}$.\n\n-   Which of these are eigenvectors of $A$?\n\n$$ v_1 = \\begin{pmatrix} 1 \\\\0 \\end{pmatrix}; \\ \\ \\ \\ \nv_2 = \\begin{pmatrix} 0 \\\\1 \\end{pmatrix}; \\ \\ \\ \\ \nv_3 = \\begin{pmatrix} 2 \\\\2 \\end{pmatrix}$$ $$v_4 = \\begin{pmatrix} 0.5 \\\\ 0.5 \\end{pmatrix};\\ \\ \\ \nv_5 = \\begin{pmatrix} 2 \\\\ 4 \\end{pmatrix};\\ \\ \\ \nv_6 = \\begin{pmatrix} 1/3 \\\\ 0 \\end{pmatrix}$$\n\n## Finding eigenvalues\n\n-   There are infinitely many eigenvectors for a matrix $A$, but at most $p$ eigenvalues.\n-   Thus, eigendecomposition starts with finding the candidate *eigenvalues*.\\\n-   An *eigenvalue* of $A$, a scalar notated $\\lambda$, satisfies the equation:\n\n$$det(A - \\lambda I) = 0$$\n\nthis is sometimes referred to as the *characteristic polynomial*.\n\n## Finding eigenvalues\n\n-   Consider matrix $A = \\begin{pmatrix} 2 & 1 \\\\ 0 & 3 \\end{pmatrix}$.\n\n-   Then:\n\n$$A  - \\lambda I = \\begin{pmatrix} 2 & 1 \\\\ 0 & 3 \\end{pmatrix}  - \\begin{pmatrix} \\lambda & 0 \\\\ 0& \\lambda \\end{pmatrix} $$\n\n. . .\n\n$$ =  \\begin{pmatrix} 2-\\lambda & 1 \\\\ 0 & 3-\\lambda \\end{pmatrix} $$\n\n. . .\n\n$$det(A-\\lambda I) = (2-\\lambda)(3-\\lambda) = 0$$\n\n. . .\n\n-   Solutions: $\\lambda = 3$ or $\\lambda = 2$\n\n## Finding eigenvectors\n\n-   Eigenvalues of $A = \\begin{pmatrix} 2 & 1 \\\\ 0 & 3 \\end{pmatrix}$ are $\\lambda = 2$ or $\\lambda = 3$.\n\n-   Thus the *eigenvectors* of $A$ are vectors that $A$ either *doubles* or *triples* in magnitude!\n\n. . .\n\n-   Reconsider the eigenvectors we identified earlier:\n\n$$ v_1 = \\begin{pmatrix} 1 \\\\0 \\end{pmatrix}; \\ \\ \\ \\ \nv_3 = \\begin{pmatrix} 2 \\\\2 \\end{pmatrix}; \\ \\ \\ \\ \nv_4 = \\begin{pmatrix} 0.5 \\\\ 0.5 \\end{pmatrix};\\ \\ \\ \nv_6 = \\begin{pmatrix} 1/3 \\\\ 0 \\end{pmatrix}$$\n\nWhich ones does $A$ double? Which ones does $A$ triple?\n\n## Finding eigenvectors for $\\lambda = 3$\n\nTo find eigenvectors for $\\lambda = 3$ (ones that $A$ triples), recall eigenvectors satisfy:\n\n$$Av = \\lambda v = 3v$$\n\n. . .\n\nIn matrix form:\n\n$$\\begin{pmatrix} 2 & 1 \\\\ 0 & 3 \\end{pmatrix}\\begin{pmatrix} x \\\\ y\\end{pmatrix} = 3 \\begin{pmatrix} x\\\\ y \\end{pmatrix}$$\n\n. . .\n\n$$ \\begin{pmatrix} 2x+y  \\\\ 0x + 3y \\end{pmatrix} = \\begin{pmatrix}3x \\\\ 3y \\end{pmatrix}$$\n\n. . .\n\n$$x = 1; y = 1$$\n\n. . .\n\nThus $v = \\begin{pmatrix} 1\\\\ 1 \\end{pmatrix},$ or any multiple of $v$, is an eigenvector that $A$ triples!\n\n## Finding eigenvectors for $\\lambda = 2$\n\nTo find eigenvectors for $\\lambda = 2$ (ones that $A$ doubles), recall eigenvectors satisfy:\n\n$$Av = \\lambda v = 2v$$\n\n. . .\n\nIn matrix form:\n\n$$\\begin{pmatrix} 2 & 1 \\\\ 0 & 3 \\end{pmatrix}\\begin{pmatrix} x \\\\ y\\end{pmatrix} = 2 \\begin{pmatrix} x\\\\ y \\end{pmatrix}$$\n\n. . .\n\n$$ \\begin{pmatrix} 2x+y  \\\\ 0x + 3y \\end{pmatrix} = \\begin{pmatrix}2x \\\\ 2y \\end{pmatrix}$$\n\n. . .\n\n$$x = 1; y = 0$$\n\n. . .\n\nThus $v = \\begin{pmatrix} 1\\\\ 0 \\end{pmatrix},$ or any multiple of $v$, is an eigenvector that $A$ doubles!\n\n## Eigenvectors of symmetric matrices are orthogonal !\n\n-   If we are decomposing a *symmetric* $p\\times p$ matrix $B$, then eigenvectors for different eigenvalues are orthogonal!\n-   Recall that orthogonal (i.e., perpendicular) vectors have dot product = 0\n\n. . .\n\n-   Recall eigenvectors for $A =\\begin{pmatrix} 2 & 1 \\\\ 0 & 3 \\end{pmatrix}$: $v_1=\\begin{pmatrix} 1\\\\ 0 \\end{pmatrix}$ and $v_2 =  \\begin{pmatrix} 1\\\\ 1 \\end{pmatrix}$. Are they orthogonal? Why/why not?\n\n. . .\n\n-   The orthogonality of eigenvectors of symmetric matrices is important for decomposing covariance matrices! (coming up)\n\n## Eigenbases\n\n-   For $\\lambda = 3$, $\\begin{pmatrix} 1  \\\\ 1 \\end{pmatrix}$ *or any multiple thereof* is an eigenvector.\n\n-   For $\\lambda = 2$, $\\begin{pmatrix} 1  \\\\ 0 \\end{pmatrix}$ *or any multiple thereof* is an eigenvector.\n\n-   Thus for any eigenvalue there are infinitely many eigenvectors.\n\n-   *Eigenbases* are unique, normed (length-1) vectors for each eigenvalue\n\n$$v_{norm} = \\frac{v}{||v||_2}$$\n\n## Eigenbase for $\\lambda = 3$\n\nL2-normed $\\lambda = 3$ eigenvector $\\begin{pmatrix} 1  \\\\ 1 \\end{pmatrix}$:\n\n$$ \\sqrt{1^2 + 1^2} = \\sqrt{2}$$\n\nNormed $\\lambda = 3$ eigenvector:\n\n$$\\begin{pmatrix} 1/\\sqrt{2}  \\\\ 1/\\sqrt{2} \\end{pmatrix} =\\begin{pmatrix} 0.7071068  \\\\ 0.7071068 \\end{pmatrix} $$\n\n## Eigenbase for $\\lambda = 2$\n\nL2-normed $\\lambda = 2$ eigenvector $\\begin{pmatrix} 1  \\\\ 0 \\end{pmatrix}$:\n\n$$ \\sqrt{1^2 + 0^2} = \\sqrt{1}$$\n\nSo $\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ already has L2 norm = 1!\n\n## `eigen` in `R`\n\nTo find eigenvalues and eigenbases in `R`:\n\n\n\n::: {.cell}\n\n```{.r .cell-code style='font-size: 2em'}\nA <- matrix(c(2,1,\n              0,3),\n            2,2, byrow=TRUE)\neigen(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```{style='font-size: 1em'}\neigen() decomposition\n$values\n[1] 3 2\n\n$vectors\n          [,1] [,2]\n[1,] 0.7071068    1\n[2,] 0.7071068    0\n```\n\n\n:::\n:::\n\n\n\n## Tying it all together\n\n-   So we have this covariance matrix (which, by the way, is symmetric, as all covariance matrices will be):\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSigma\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         x        y\nx 1.276887 2.946598\ny 2.946598 8.774893\n```\n\n\n:::\n:::\n\n\n\n. . .\n\n-   The *eigenbases* of $\\Sigma$ are the new axes we want!!\n-   And, since $\\Sigma$ is symmetric, they are guaranteed orthogonal (so will work as new axes).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\neig_decomposition <- eigen(Sigma)\neig_decomposition\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\neigen() decomposition\n$values\n[1] 9.7942712 0.2575082\n\n$vectors\n          [,1]       [,2]\n[1,] 0.3269394 -0.9450453\n[2,] 0.9450453  0.3269394\n```\n\n\n:::\n:::\n\n\n\n## Plotting the bases\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\neigenvectors <- eig_decomposition$vectors\n\nvectordf <- data.frame(t(eigenvectors)) %>% \n  mutate(mean_x = meanvec[1], mean_y = meanvec[2]) %>% \n  rename(evector_xdirection = X1, evector_ydirection = X2)\n\nvectordf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  evector_xdirection evector_ydirection   mean_x   mean_y\n1          0.3269394          0.9450453 4.004742 3.075866\n2         -0.9450453          0.3269394 4.004742 3.075866\n```\n\n\n:::\n\n```{.r .cell-code}\neigenbaseplot <- base + \n  geom_point(aes(x = x, y = y), data = df, alpha = 0.8, size = 3) + \n  geom_point(aes(x = mean_x,  y = mean_y), data = vectordf, color='red',size = 4) + \n  geom_segment(aes(x = mean_x, y = mean_y, \n                   xend = mean_x +evector_xdirection, \n                   yend = mean_y + evector_ydirection), \n               color='red', data = vectordf,  \n               arrow = arrow(length = unit(0.2, \"cm\")), linewidth = 1) \neigenbaseplot\n```\n\n::: {.cell-output-display}\n![](1.3---covariance-and-eigendecomposition_files/figure-revealjs/unnamed-chunk-24-1.png){fig-align='center' width=576}\n:::\n:::\n\n\n\n## Scaling the bases\n\nIn this plot we see the *directions* of the axes:\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](1.3---covariance-and-eigendecomposition_files/figure-revealjs/unnamed-chunk-25-1.png){fig-align='center' width=576}\n:::\n:::\n\n\n\n. . .\n\nThe eigenvalues give us a sense as to how much variability is in the *direction* of each axis:\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 9.7942712 0.2575082\n```\n\n\n:::\n:::\n\n\n\n. . .\n\nThese values indicate there is a lot more variability in the $e_1$ (first eigenvector) direction than in the $e_2$ (second eigenvector) direction.\n\n## Scaling the bases\n\nWe can scale the axes accordingly by the square root of the eigenvalues:\n\n::: column\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvectordf <- data.frame(t(eigenvectors)) %>% \n  mutate(mean_x = meanvec[1], mean_y = meanvec[2]) %>% \n  rename(evector_xdirection = X1, evector_ydirection = X2) %>% \n  mutate(evector_xdirection_scaled = sqrt(eigenvalues)*evector_xdirection,\n         evector_ydirection_scaled = sqrt(eigenvalues)*evector_ydirection)\n\nvectordf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  evector_xdirection evector_ydirection   mean_x   mean_y\n1          0.3269394          0.9450453 4.004742 3.075866\n2         -0.9450453          0.3269394 4.004742 3.075866\n  evector_xdirection_scaled evector_ydirection_scaled\n1                 1.0231830                 2.9575949\n2                -0.4795658                 0.1659063\n```\n\n\n:::\n:::\n\n\n:::\n\n::: column\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](1.3---covariance-and-eigendecomposition_files/figure-revealjs/unnamed-chunk-28-1.png){fig-align='center' width=576}\n:::\n:::\n\n\n:::\n\n## Interpreting the eigenvalues\n\n-   The eigenvalues also have meaningful interpretations in terms of variability.\n\n-   Recall:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSigma\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         x        y\nx 1.276887 2.946598\ny 2.946598 8.774893\n```\n\n\n:::\n\n```{.r .cell-code}\nsum(diag(Sigma))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 10.05178\n```\n\n\n:::\n:::\n\n\n\n... so the total horizontal and vertical variance is 10.0517794.\n\n. . .\n\nNow consider the eigenvalues:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\neigenvalues\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 9.7942712 0.2575082\n```\n\n\n:::\n\n```{.r .cell-code}\nsum(eigenvalues)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 10.05178\n```\n\n\n:::\n:::\n\n\n\n... so the summed eigenvalues *also* equal the total horizontal and vertical variance!!\n\n## Eigenvectors as rotation tools\n\n-   Reconsider our mean-centered data:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_centered <- scale(df, center = TRUE, scale = FALSE)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](1.3---covariance-and-eigendecomposition_files/figure-revealjs/unnamed-chunk-32-1.png){fig-align='center' width=576}\n:::\n:::\n\n\n\n## Eigenvectors as rotation tools\n\n-   Multiplying the centered data by the eigenvectors yields the *rotation* that treats the eigenvectors as the new horizontal and vertical axes:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_rotated <- data.frame(df_centered %*% eigenvectors)\nnames(df_rotated) <- c('x','y')\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](1.3---covariance-and-eigendecomposition_files/figure-revealjs/unnamed-chunk-34-1.png){fig-align='center' width=576}\n:::\n:::\n\n\n\n## Eigenvalues as new covariance diagonals\n\n-   Let's look at the covariance matrix of the centered, rotated data:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncov(df_rotated) %>% round(4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       x      y\nx 9.7943 0.0000\ny 0.0000 0.2575\n```\n\n\n:::\n:::\n\n\n\n. . .\n\n-   Hopefully these look familiar:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\neigenvalues\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 9.7942712 0.2575082\n```\n\n\n:::\n:::\n\n\n\n-   Thus the eigenvalues tell us the total horizontal and vertical variability of the mean centered data rotated to orient along the eigenvectors!\n\n## Summary\n\n-   Covariance matrix $\\Sigma$: symmetric $p\\times p$ matrix\n    -   Marginal variance of each dimension on the diagonal\n    -   Covariance on the off-diagonals\n\n. . .\n\n-   Eigenvectors of a square $p\\times p$ matrix $A$\n    -   Vectors that $A$ scales by a constant\n    -   Can be normalized to determine *eigenbases*\n    -   If $A$ is symmetric, eigenvectors of different eigenvalues are orthogonal\n\n. . .\n\n-   Eigenvalues\n    -   Scalars that tell us how much eigenvectors are scaled by $A$\n\n## Summary\n\nEigendecomposition of $\\Sigma$:\n\n-   Eigenvectors\n\n    1.  tell us which directions the reoriented axes go;\n    2.  give us rules for rotating the data to be uncorrelated and reoriented;\n    3.  are orthogonal (since $\\Sigma$ is symmetric)\n-   Eigenvalues tell us the values of marginal horizontal and vertical variance after rotation\n\n\n## Correlation vs covariance\n\n-   Consider the following two covariance matrices from two data sets, `df1` and `df2`.\n-   In both, we know the covariance but not the marginal (horizontal/vertical) variances:\n\n$$ \\Sigma_1 = \\begin{pmatrix}? & 50 \\\\ 50 & ?\\end{pmatrix};\\ \\ \\  \\Sigma_2 = \\begin{pmatrix}? & 0.1 \\\\ 0.1 & ?\\end{pmatrix}$$\n\n-   Which matrix represents a dataset with more relationship between the two columns?\n\n## Plotting the two data sets\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](1.3---covariance-and-eigendecomposition_files/figure-revealjs/unnamed-chunk-37-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n-   Which data set has more relationship?\\\n-   What differences do you notice about between these two data sets?\n\n## Covariance vs correlation\n\n-   Let $\\sigma_1^2$ represent the marginal variance in the first dimension (e.g., horizontal)\n-   Let $\\sigma_2^2$ represent the marginal variance in the second dimension (e.g., vertical)\n-   The *correlation*, $\\rho$, between two variables $x$ and $y$ is defined as:\n\n$$\\rho = \\frac{Cov(x,y)}{\\sigma_1 \\sigma_2}$$\n\n. . .\n\n-   Note that this also implies that $Cov(x,y) = \\rho \\sigma_1 \\sigma_2$; in other words, that the covariance is a function both of the *relationship* between variables and the *marginal* variances!\n\n## Facts about $\\rho$\n\n-   Unitless\n-   $-1 \\leq \\rho \\leq 1$\n-   $|\\rho|$ close to 1 $\\rightarrow$ strong relationship\n-   $|\\rho|$ close to 0 $\\rightarrow$ weak relationship\n\n## Covariance - correlation relationship\n\n::: {.column width=\"40%\"}\nFor 2D data:\n\n$$\\Sigma = \\begin{pmatrix}\\sigma_1^2 & \\sigma_1 \\sigma_2 \\rho\\\\\n\\sigma_1 \\sigma_2 \\rho & \\sigma_2^2 \\end{pmatrix}$$\n\n$$R = \\begin{pmatrix}1 &  \\rho\\\\\n\\rho & 1 \\end{pmatrix}$$\n:::\n\n::: {.column .fragment width=\"60%\"}\nFor $p$-dimensional data:\n\n$$\\Sigma = \\begin{pmatrix}\\sigma_1^2 & \\sigma_1 \\sigma_2 \\rho_{12} & \\sigma_1 \\sigma_3 \\rho_{13} & ... & \\sigma_1 \\sigma_p \\rho_{1p}\\\\\n\\sigma_1 \\sigma_2 \\rho_{12} & \\sigma_2^2 & \\sigma_2 \\sigma_3 \\rho_{23} & ... & \\sigma_2 \\sigma_p \\rho_{2p}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\ \n\\sigma_1 \\sigma_p \\rho_{1p} & \\sigma_2\\sigma_p \\rho_{2p} & \\sigma_3 \\sigma_p \\rho_{3p} & ... & \\sigma_p^2\\end{pmatrix}$$\n\n$$R = \\begin{pmatrix}1 &  \\rho_{12} & \\rho_{13} & ... & \\rho_{1p}\\\\\n\\rho_{12} & 1 & \\rho_{23} & ... & \\rho_{2p}\\\\ \n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n\\rho_{1p} & \\rho_{2p} & \\rho_{3p} & ... & 1\\end{pmatrix}$$\n:::\n\n## Find the correlation\n\nThe two covariance matrices I used to simulate `df1` and `df2` were:\n\n$$ \\Sigma_1 = \\begin{pmatrix}100 & 50 \\\\ 50 & 2500\\end{pmatrix};\\ \\ \\  \\Sigma_2 = \\begin{pmatrix}1/9 & 0.1 \\\\ 0.1 & 1/9\\end{pmatrix}$$\n\n-   What is $\\rho_1$, the correlation between the two variables for `df1`?\n-   What is $\\rho_2$, the correlation between the two variables for `df2`?\n\n## Simulated data sets\n\n::: {.column width=\"60%\"}\nCode I used for simulating the two data sets:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Parameters:\nsigx <- 10; sigy <- 50; rho <- 0.1\nSigma1 <- matrix(c(sigx^2, sigx*sigy*rho,\n                   sigx*sigy*rho,sigy^2), nrow=2)\n\nsigx <- 1/3; sigy <- 1/3; rho <- .9\nSigma2 <- matrix(c(sigx^2, sigx*sigy*rho,\n                   sigx*sigy*rho,sigy^2), nrow=2)\n\n# Simulate data\nlibrary(MASS)\nset.seed(944)\ndf1 <- data.frame(mvrnorm(n = 50, mu = c(0,0), Sigma1))\nnames(df1)=c(\"X\",\"Y\")\ndf2 <- data.frame(mvrnorm(n = 50, mu = c(0,0), Sigma2))\nnames(df2)=c(\"X\",\"Y\")\n```\n:::\n\n\n:::\n\n::: {.column .fragment width=\"40%\"}\nFinding the covariance and correlation matrices:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncov(df1) %>% round(3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       X        Y\nX 80.744   11.723\nY 11.723 2750.287\n```\n\n\n:::\n\n```{.r .cell-code}\ncor(df1) %>% round(3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      X     Y\nX 1.000 0.025\nY 0.025 1.000\n```\n\n\n:::\n\n```{.r .cell-code}\ncov(df2) %>% round(3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      X     Y\nX 0.121 0.110\nY 0.110 0.118\n```\n\n\n:::\n\n```{.r .cell-code}\ncor(df2) %>% round(3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      X     Y\nX 1.000 0.923\nY 0.923 1.000\n```\n\n\n:::\n:::\n\n\n:::\n\n## Correlation = covariance of scaled data!\n\n-   It turns out that $R$ for a data set is just $\\Sigma$ if we scale the data first!\n\n::: column\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Correlation of unscaled data:\ncor(df1) %>% round(3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      X     Y\nX 1.000 0.025\nY 0.025 1.000\n```\n\n\n:::\n\n```{.r .cell-code}\n#Scale the data:\ndf1_scaled <- scale(df1)\n\n#Covariance of scaled data:\ncov(df1_scaled) %>% round(3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      X     Y\nX 1.000 0.025\nY 0.025 1.000\n```\n\n\n:::\n:::\n\n\n:::\n\n::: column\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Correlation of unscaled data:\ncor(df2) %>% round(3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      X     Y\nX 1.000 0.923\nY 0.923 1.000\n```\n\n\n:::\n\n```{.r .cell-code}\n#Scale the data:\ndf2_scaled <- scale(df2)\n\n#Covariance of scaled data:\ncov(df2_scaled) %>% round(3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      X     Y\nX 1.000 0.923\nY 0.923 1.000\n```\n\n\n:::\n:::\n\n\n:::\n\n## Implications for eigendecomposition\n\n-   $\\Sigma$:\n    -   Eigenvectors tell us the coordinates of the new axes in original space (or equivalently, how to rotate the data in original coordinates)\n    -   Eigenvalues sum to total diagonals of $\\Sigma$ (total marginal variances)\n\n. . .\n\n-   $R$:\n    -   Eigenvectors tell us the coordinates of the new axes in scaled space (or equivalently, how to rotate the scaled data)\n    -   Eigenvalues sum to total diagonals of $R$\n    -   **What will eigenvalues sum to if doing eigendecomposition of any** $p\\times p$ correlation matrix?\n\n## `df1`\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n::: column\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](1.3---covariance-and-eigendecomposition_files/figure-revealjs/unnamed-chunk-43-1.png){fig-align='center' width=336}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\neigen(cov(df1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\neigen() decomposition\n$values\n[1] 2750.33837   80.69275\n\n$vectors\n            [,1]         [,2]\n[1,] 0.004391318 -0.999990358\n[2,] 0.999990358  0.004391318\n```\n\n\n:::\n:::\n\n\n:::\n\n::: column\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](1.3---covariance-and-eigendecomposition_files/figure-revealjs/unnamed-chunk-45-1.png){fig-align='center' width=336}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\neigen(cor(df1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\neigen() decomposition\n$values\n[1] 1.0248771 0.9751229\n\n$vectors\n          [,1]       [,2]\n[1,] 0.7071068 -0.7071068\n[2,] 0.7071068  0.7071068\n```\n\n\n:::\n:::\n\n\n:::\n\n## `df2`\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n::: column\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](1.3---covariance-and-eigendecomposition_files/figure-revealjs/unnamed-chunk-48-1.png){fig-align='center' width=336}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\neigen(cov(df2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\neigen() decomposition\n$values\n[1] 0.228947488 0.009138127\n\n$vectors\n           [,1]       [,2]\n[1,] -0.7118287  0.7023531\n[2,] -0.7023531 -0.7118287\n```\n\n\n:::\n:::\n\n\n:::\n\n::: column\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](1.3---covariance-and-eigendecomposition_files/figure-revealjs/unnamed-chunk-50-1.png){fig-align='center' width=336}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\neigen(cor(df2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\neigen() decomposition\n$values\n[1] 1.92322439 0.07677561\n\n$vectors\n          [,1]       [,2]\n[1,] 0.7071068 -0.7071068\n[2,] 0.7071068  0.7071068\n```\n\n\n:::\n:::\n\n\n:::\n\n",
    "supporting": [
      "1.3---covariance-and-eigendecomposition_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}